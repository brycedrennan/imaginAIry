{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ImaginAIry \ud83e\udd16\ud83e\udde0","text":"<p>Pythonic AI generation of images and videos</p> <p> </p> <p> </p>"},{"location":"#features","title":"Features","text":"<ul> <li>Image Generation: Create with SDXL, Openjourney, OpenDalle, and many others.<ul> <li>Generation Control: Exert detailed control over the generation process.</li> </ul> </li> <li>Image Editing: Edit images with instructions.</li> <li>Image Upscaling: Add details to images.</li> <li>Video Generation: Turn images into videos.</li> <li>Image Captioning: </li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># on macOS, make sure rust is installed first\n# be sure to use Python 3.10, Python 3.11 is not supported at the moment\npip install imaginairy\n</code></pre>"},{"location":"#image-generation","title":"Image Generation","text":"CLIPython <pre><code>aimg imagine --seed 1 --model sdxl --size hd \"wide shot of sun setting on a green valley\"\n</code></pre> <pre><code>from imaginairy.api.generate import imagine\nfrom imaginairy.schema import ImaginePrompt\n\nprompt = ImaginePrompt(\n    prompt=\"wide shot of sun setting on a green valley\", \n    seed=1, \n    model_weights=\"sdxl\", \n    size=\"hd\"\n)\n\nresult = next(imagine(prompts=prompt))\nresult.img.save(\"sun_set_green_valley.png\")\n</code></pre>"},{"location":"#image-generation-control","title":"Image Generation Control","text":"<p>Guide the generation process by providing body poses, depth maps, canny edges, hed boundaries, normal maps, or even QR codes.</p>"},{"location":"#body-pose-control","title":"Body Pose Control","text":"CLIPython <pre><code>imagine --control-image assets/indiana.jpg  --control-mode openpose --caption-text openpose \"photo of a polar bear\"\n</code></pre> <pre><code>from imaginairy.api.generate import imagine\nfrom imaginairy.schema import ImaginePrompt, ControlInput, LazyLoadingImage\n\nimage = LazyLoadingImage(filepath=\"assets/indiana.jpg\")\ncontrol_mode = ControlInput(mode=\"openpose\", image=image)\nprompt = ImaginePrompt(prompt=\"photo of a polar bear\", control_inputs=[control_mode], seed=1)\n\nresult = next(imagine(prompts=prompt))\nresult.img.save(\"assets/indiana-pose-polar-bear.jpg\")\n</code></pre>"},{"location":"#canny-edge-control","title":"Canny Edge Control","text":"CLIPython <pre><code>imagine --control-image assets/lena.png  --control-mode canny \"photo of a woman with a hat looking at the camera\"\n</code></pre> <pre><code>from imaginairy.api.generate import imagine\nfrom imaginairy.schema import ImaginePrompt, ControlInput, LazyLoadingImage\n\nimage = LazyLoadingImage(filepath=\"assets/lena.png\")\ncontrol_mode = ControlInput(mode=\"canny\", image=image)\nprompt = ImaginePrompt(prompt=\"photo of a woman with a hat looking at the camera\", control_inputs=[control_mode], seed=1)\n\nresult = next(imagine(prompts=prompt))\nresult.img.save(\"assets/lena-canny-generated.jpg\")\n</code></pre>"},{"location":"#hed-boundary-control","title":"HED Boundary Control","text":"CLIPython <pre><code>imagine --control-image dog.jpg  --control-mode hed  \"photo of a dalmation\"\n</code></pre> <pre><code>from imaginairy.api.generate import imagine\nfrom imaginairy.schema import ImaginePrompt, ControlInput, LazyLoadingImage\n\nimage = LazyLoadingImage(filepath=\"assets/000032_337692011_PLMS40_PS7.5_a_photo_of_a_dog.jpg\")\ncontrol_mode = ControlInput(mode=\"hed\", image=image)\nprompt = ImaginePrompt(prompt=\"photo of a dalmation\", control_inputs=[control_mode], seed=1)\n\nresult = next(imagine(prompts=prompt))\nresult.img.save(\"assets/dog-hed-boundary-dalmation.jpg\")\n</code></pre>"},{"location":"#depth-map-control","title":"Depth Map Control","text":"CLIPython <pre><code>imagine --control-image fancy-living.jpg  --control-mode depth  \"a modern living room\"\n</code></pre> <pre><code>from imaginairy.api.generate import imagine\nfrom imaginairy.schema import ImaginePrompt, ControlInput, LazyLoadingImage\n\nimage = LazyLoadingImage(filepath=\"assets/fancy-living.jpg\")\ncontrol_mode = ControlInput(mode=\"depth\", image=image)\nprompt = ImaginePrompt(prompt=\"a modern living room\", control_inputs=[control_mode], seed=1)\n\nresult = next(imagine(prompts=prompt))\nresult.img.save(\"assets/fancy-living-depth-generated.jpg\")\n</code></pre>"},{"location":"#normal-map-control","title":"Normal Map Control","text":"CLIPython <pre><code>imagine --control-image bird.jpg  --control-mode normal  \"a bird\"\n</code></pre> <pre><code>from imaginairy.api.generate import imagine\nfrom imaginairy.schema import ImaginePrompt, ControlInput, LazyLoadingImage\n\nimage = LazyLoadingImage(filepath=\"assets/013986_1_kdpmpp2m59_PS7.5_a_bluejay_[generated].jpg\")\ncontrol_mode = ControlInput(mode=\"normal\", image=image)\nprompt = ImaginePrompt(prompt=\"a bird\", control_inputs=[control_mode], seed=1)\n\nresult = next(imagine(prompts=prompt))\nresult.img.save(\"assets/bird-normal-generated.jpg\")\n</code></pre>"},{"location":"#image-shuffle-control","title":"Image Shuffle Control","text":"<p>Generates the image based on elements of the control image. Kind of similar to style transfer.</p> CLIPython <pre><code>imagine --control-image pearl-girl.jpg  --control-mode shuffle  \"a clown\"\n</code></pre> <pre><code>from imaginairy.api.generate import imagine\nfrom imaginairy.schema import ImaginePrompt, ControlInput, LazyLoadingImage\n\nimage = LazyLoadingImage(filepath=\"assets/girl_with_a_pearl_earring.jpg\")\ncontrol_mode = ControlInput(mode=\"shuffle\", image=image)\nprompt = ImaginePrompt(prompt=\"a clown\", control_inputs=[control_mode], seed=1)\n\nresult = next(imagine(prompts=prompt))\nresult.img.save(\"assets/pearl_shuffle_clown_019331_1_kdpmpp2m15_PS7.5_img2img-0.0_a_clown.jpg\")\n</code></pre> <p>The middle image is the \"shuffled\" input image</p> <p> </p>"},{"location":"#editing-instructions-control","title":"Editing Instructions Control","text":"<p>Similar to instructPix2Pix (below) but works with any SD 1.5 based model.</p> CLIPython <pre><code>imagine --control-image pearl-girl.jpg  --control-mode edit --init-image-strength 0.01 --steps 30  --negative-prompt \"\" --model openjourney-v2 \"make it anime\" \"make it at the beach\" \n</code></pre> <pre><code>from imaginairy.api.generate import imagine\nfrom imaginairy.schema import ImaginePrompt, ControlInput, LazyLoadingImage\n\nimage = LazyLoadingImage(filepath=\"assets/girl_with_a_pearl_earring.jpg\")\ncontrol_mode = ControlInput(mode=\"edit\", image=image)\n\nprompts = [ImaginePrompt(prompt=\"make it anime\", control_inputs=[control_mode], init_image_strength=0.01, steps=30, negative_prompt=\"\", model_weights=\"openjourney-v2\"),\n            ImaginePrompt(prompt=\"make it at the beach\", control_inputs=[control_mode], init_image_strength=0.01, steps=30, negative_prompt=\"\", model_weights=\"openjourney-v2\")]\n\nimagine_iterator = imagine(prompts=prompts)\n\nresult = next(imagine_iterator)\nresult.img.save(\"assets/pearl_anime_019537_521829407_kdpmpp2m30_PS9.0_img2img-0.01_make_it_anime.jpg\")\n\nresult = next(imagine_iterator)\nresult.img.save(\"assets/pearl_beach_019561_862735879_kdpmpp2m30_PS7.0_img2img-0.01_make_it_at_the_beach.jpg\")\n</code></pre> <p> </p>"},{"location":"#add-details-control-upscalingsuper-resolution","title":"Add Details Control (upscaling/super-resolution)","text":"<p>Replaces existing details in an image. Good to use with --init-image-strength 0.2</p> CLIPython <pre><code>imagine --control-image \"assets/wishbone.jpg\" --control-mode details \"sharp focus, high-resolution\" --init-image-strength 0.2 --steps 30 --size 2048x2048 \n</code></pre> <pre><code>from imaginairy.api.generate import imagine\nfrom imaginairy.schema import ImaginePrompt, ControlInput, LazyLoadingImage\n\nimage = LazyLoadingImage(filepath=\"assets/wishbone_headshot_badscale.jpg\")\ncontrol_mode = ControlInput(mode=\"details\", image=image)\nprompt = ImaginePrompt(prompt=\"sharp focus, high-resolution\", control_inputs=[control_mode], init_image_strength=0.2)\n\nresult = next(imagine(prompts=prompt))\nresult.img.save(\"assets/wishbone_headshot_details.jpg\")\n</code></pre> <p> </p>"},{"location":"#image-recolorization-using-brightness-control","title":"Image (re)Colorization (using brightness control)","text":"<p>Colorize black and white images or re-color existing images.</p> <p>The generated colors will be applied back to the original image. You can either provide a caption or  allow the tool to generate one for you.</p> CLIPython <pre><code>aimg colorize pearl-girl.jpg --caption \"photo of a woman\"\n</code></pre> <pre><code>from imaginairy.api.colorize import colorize_img\nfrom PIL import Image, ImageEnhance, ImageStat\n\ninit_image = Image.open(\"assets/girl_with_a_pearl_earring.jpg\")\nimage = colorize_img(img=init_image, caption=\"photo of a woman\")\nimage.save(\"assets/pearl-colorized.jpg\")\n</code></pre> <p> </p>"},{"location":"#image-upscaling","title":"Image Upscaling","text":"<p>Upscale images easily.</p> CLIPython <pre><code>aimg upscale assets/000206_856637805_PLMS40_PS7.5_colorful_smoke.jpg --upscale-model real-hat\n</code></pre> <pre><code>from imaginairy.api.upscale import upscale\n\nimg = upscale(img=\"assets/000206_856637805_PLMS40_PS7.5_colorful_smoke.jpg\")\nimg.save(\"colorful_smoke.upscaled.jpg\")\n</code></pre> <p> \u27a1\ufe0f  </p> <p>Upscaling uses Spandrel to make it easy to use different upscaling models. You can view different integrated models by running <code>aimg upscale --list-models</code>, and then use it with <code>--upscale-model &lt;model-name&gt;</code>. Also accepts url's if you want to upscale an image with a different model. Control the new file format/location with --format.</p>"},{"location":"#video-generation","title":"Video Generation","text":"CLIPython <pre><code>aimg videogen --start-image assets/rocket-wide.png\n</code></pre> <pre><code>from imaginairy.api.video_sample import generate_video\n\ngenerate_video(input_path=\"assets/rocket-wide.png\")\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"ChangeLog","text":"<p>14.3.0</p> <ul> <li>feature: integrates spandrel for upscaling </li> <li>fix: allow loading sdxl models from local paths. </li> </ul> <p>14.2.0 - \ud83c\udf89 feature: add image prompt support via <code>--image-prompt</code> and <code>--image-prompt-strength</code> (IPAdapter)</p> <p>14.1.1 - tests: add installation tests for windows, mac, and conda - fix: dependency issues</p> <p>14.1.0</p> <ul> <li>\ud83c\udf89 feature: make video generation smooth by adding frame interpolation</li> <li>feature: SDXL weights in the compvis format can now be used</li> <li>feature: allow video generation at any size specified by user</li> <li>feature: video generations output in \"bounce\" format</li> <li>feature: choose video output format: mp4, webp, or gif</li> <li>feature: fix random seed handling in video generation</li> <li>docs: auto-publish docs on push to master</li> <li>build: remove imageio dependency</li> <li>build: vendorize facexlib so we don't install its unneeded dependencies</li> </ul> <p>14.0.4</p> <ul> <li>docs: add a documentation website at https://brycedrennan.github.io/imaginAIry/</li> <li>build: remove fairscale dependency</li> <li>fix: video generation was broken</li> </ul> <p>14.0.3</p> <ul> <li>fix: several critical bugs with package</li> <li>tests: add a wheel smoketest to detect these issues in the future</li> </ul> <p>14.0.0</p> <ul> <li>\ud83c\udf89 video generation using Stable Video Diffusion</li> <li>add <code>--videogen</code> to any image generation to create a short video from the generated image</li> <li>or use <code>aimg videogen</code> to generate a video from an image</li> <li>\ud83c\udf89 SDXL (Stable Diffusion Extra Large) models are now supported.</li> <li>try <code>--model opendalle</code> or <code>--model sdxl</code></li> <li>inpainting and controlnets are not yet supported for SDXL</li> <li>\ud83c\udf89 imaginairy is now backed by the refiners library</li> <li>This was a huge rewrite which is why some features are not yet supported.  On the plus side, refiners supports cutting edge features (SDXL, image prompts, etc) which will be added to imaginairy soon.</li> <li>self-attention guidance which makes details of images more accurate</li> <li>\ud83c\udf89 feature: larger image generations now work MUCH better and stay faithful to the same image as it looks at a smaller size.  For example <code>--size 720p --seed 1</code> and <code>--size 1080p --seed 1</code> will produce the same image for SD15</li> <li>\ud83c\udf89 feature: loading diffusers based models now supported. Example <code>--model https://huggingface.co/ainz/diseny-pixar --model-architecture sd15</code></li> <li>\ud83c\udf89 feature: qrcode controlnet!</li> <li>feature: generate word images automatically. great for use with qrcode controlnet: <code>imagine \"flowers\" --gif --size hd --control-mode qrcode --control-image \"textimg='JOY' font_color=white background_color=gray\" -r 10</code></li> <li>feature: opendalle 1.1 added. <code>--model opendalle</code> to use it</li> <li>feature: added <code>--size</code> parameter for more intuitive sizing (e.g. 512, 256x256, 4k, uhd, FHD, VGA, etc)</li> <li>feature: detect if wrong torch version is installed and provide instructions on how to install proper version</li> <li>feature: better logging output: color, error handling</li> <li>feature: support for pytorch 2.0</li> <li>feature: command line output significantly cleaned up and easier to read</li> <li>feature: adds --composition-strength parameter to cli (#416)</li> <li>performance: lower memory usage for upscaling</li> <li>performance: lower memory usage at startup</li> <li>performance: add sliced attention to several models (lowers memory use)</li> <li>fix: simpler memory management that avoids some of the previous bugs</li> <li>deprecated: support for python 3.8, 3.9</li> <li>deprecated: support for torch 1.13</li> <li>deprecated: support for Stable Diffusion versions 1.4, 2.0, and 2.1</li> <li>deprecated: image training</li> <li>broken: samplers other than ddim</li> </ul> <p>13.2.1</p> <ul> <li>fix: pydantic models for http server working now. Fixes #380</li> <li>fix: install triton so annoying message is gone</li> </ul> <p>13.2.0</p> <ul> <li>fix: allow tile_mode to be set to True or False for backward compatibility</li> <li>fix: various pydantic issues have been resolved</li> <li>feature: switch to pydantic 2.3 (faster but was a pain to migrate)</li> </ul> <p>13.1.0</p> <ul> <li>feature: api server now has feature parity with the python API. View the docs at http://127.0.0.1:8000/docs after running <code>aimg server</code></li> <li><code>ImaginePrompt</code> is now a pydantic model and can thus be sent over the rest API</li> <li>images are expected in base64 string format</li> <li>fix: pin pydantic to 2.0 for now</li> <li>build: better python 3.11 incompatibility messaging (fixes #342)</li> <li>build: add minimum versions to requirements to improve dependency resolution</li> <li>docs: add a discord link</li> </ul> <p>13.0.1</p> <ul> <li>feature: show full stack trace when there is an api error</li> <li>fix: make lack of support for python 3.11 explicit</li> <li>fix: add some routes to match StableStudio routes</li> </ul> <p>13.0.0</p> <ul> <li>\ud83c\udf89 feature: multi-controlnet support. pass in multiple <code>--control-mode</code>, <code>--control-image</code>, and <code>--control-image-raw</code> arguments.</li> <li>\ud83c\udf89 feature: add colorization controlnet. improve <code>aimg colorize</code> command</li> <li>\ud83c\udf89\ud83e\uddea feature: Graphical Web Interface StableStudio. run <code>aimg server</code> and visit http://127.0.0.1:8000/</li> <li>\ud83c\udf89\ud83e\uddea feature: API server <code>aimg server</code> command. Runs a http webserver (not finished). After running, visit http://127.0.0.1:8000/docs for api.</li> <li>\ud83c\udf89\ud83e\uddea feature: API support for Stablity AI's new open-source Generative AI interface, StableStudio.</li> <li>\ud83c\udf89\ud83e\uddea feature: \"better\" memory management. If GPU is full, least-recently-used model is moved to RAM. I'm not confident this works well.</li> <li>feature: [disabled] inpainting controlnet can be used instead of finetuned inpainting model</li> <li>The inpainting controlnet doesn't work as well as the finetuned model</li> <li>feature: python interface allows configuration of controlnet strength</li> <li>feature: show full stack trace on error in cli</li> <li>fix: hide the \"triton\" error messages</li> <li>fix: package will not try to install xformers on <code>aarch64</code> machines. While this will allow the dockerfile to build on  MacOS M1, torch will not be able to use the M1 when generating images.</li> <li>build: specify proper Pillow minimum version (fixes #325)</li> <li>build: check for torch version at runtime (fixes #329)</li> </ul> <p>12.0.3</p> <ul> <li>fix: exclude broken versions of timm as dependencies</li> </ul> <p>12.0.2</p> <ul> <li>fix: move normal map preprocessor for conda compatibility</li> </ul> <p>12.0.1</p> <ul> <li>fix: use correct device for depth images on mps. Fixes #300</li> </ul> <p>12.0.0</p> <ul> <li>\ud83c\udf89 feature: add \"detail\" control mode.  Add details to an image. Great for upscaling an image.</li> <li>\ud83c\udf89 feature: add \"edit\" control mode.  Edit images using text instructions with any SD 1.5 based model. Similar to instructPix2Pix.</li> <li>\ud83c\udf89 feature: add \"shuffle\" control mode. Image is generated from elements of control image. Similar to style transfer.</li> <li>\ud83c\udf89 feature: upgrade to controlnet 1.1</li> <li>\ud83c\udf89 fix: controlnet now works with all SD 1.5 based models</li> <li>feature: add openjourney-v4</li> <li>fix: raw control images are now properly loaded. fixes #296</li> <li>fix: filenames start numbers after latest image, even if some previous images were deleted</li> </ul> <p>11.1.1</p> <ul> <li>fix: fix globbing bug with input image path handling</li> <li>fix: changed sample to True to generate caption using blip model</li> </ul> <p>11.1.0</p> <ul> <li>docs: add some example use cases</li> <li>feature: add art-scene, desktop-background, interior-style, painting-style phraselists</li> <li>fix: compilation animations create normal slideshows instead of \"bounces\"</li> <li>fix: file globbing works in the interactive shell</li> <li>fix: fix model downloads that were broken by library change in transformers 4.27.0</li> </ul> <p>11.0.0</p> <ul> <li>all these changes together mean same seed/sampler will not be guaranteed to produce same image (thus the version bump)</li> <li>fix: image composition didn't work very well. Works well now but probably very slow on non-cuda platforms</li> <li>fix: remove upscaler tiling message</li> <li>fix: improve k-diffusion sampler schedule. significantly improves image quality of default sampler</li> <li>fix: img2img was broken for all samplers except plms and ddim when init image strength was &gt;~0.25</li> </ul> <p>10.2.0</p> <ul> <li>feature: input raw control images (a pose, canny map, depth map, etc) directly using <code>--control-image-raw</code>    This is opposed to current behavior of extracting the control signal from an input image via <code>--control-image</code></li> <li>feature: <code>aimg model-list</code> command lists included models</li> <li>feature: system memory added to <code>aimg system-info</code> command</li> <li>feature: add <code>--fix-faces</code> options to <code>aimg upscale</code> command</li> <li>fix: add missing metadata attributes to generated images</li> <li>fix: image composition step was producing unnecessarily blurry images </li> <li>refactor: split <code>aimg</code> cli code into multiple files</li> <li>docs: pypi docs now link properly to github automatically</li> </ul> <p>10.1.0</p> <ul> <li>feature: \ud83c\udf89 ControlNet integration!  Control the structure of generated images.</li> <li>feature: <code>aimg colorize</code> attempts to use controlnet to colorize images</li> <li>feature: <code>--caption-text</code> command adds text at the bottom left of an image</li> </ul> <p>10.0.1</p> <ul> <li>fix: <code>edit</code> was broken</li> </ul> <p>10.0.0</p> <ul> <li>feature: \ud83c\udf89\ud83c\udf89 Make large images while retaining composition. Try <code>imagine \"a flower\" -w 1920 -h 1080</code></li> <li>fix: create compilations directory automatically</li> <li>perf: sliced encoding of images to latents (removes memory bottleneck)</li> <li>perf: use Silu for performance improvement over nonlinearity</li> <li>perf: <code>xformers</code> added as a dependency for linux and windows.  Gives a nice speed boost.</li> <li>perf: sliced attention now runs on MacOS. A typo prevented that from happening previously.</li> <li>perf: sliced latent decoding - now possible to make much bigger images. 3310x3310 on 11 GB GPU.</li> </ul> <p>9.0.2</p> <ul> <li>fix: edit interface was broken</li> </ul> <p>9.0.1</p> <ul> <li>fix: use entry_points for windows since setup.py scripts doesn't work on windows #239</li> </ul> <p>9.0.0</p> <ul> <li>perf: cli now has minimal overhead such that <code>aimg --help</code> runs in ~650ms instead of ~3400ms</li> <li>feature: <code>edit</code> and <code>imagine</code> commands now accept multiple images (which they will process separately).  This allows  batch editing of images as requested in #229</li> <li>refactor: move <code>--surprise-me</code> to its own subcommand <code>edit-demo</code></li> <li>feature: allow selection of output image format with <code>--output-file-extension</code></li> <li>docs: make training fail on MPS platform with useful error message</li> <li>docs: add directions on how to change model cache path</li> </ul> <p>8.3.1</p> <ul> <li>fix: init-image-strength type</li> </ul> <p>8.3.0</p> <ul> <li>feature: create <code>gifs</code> or <code>mp4s</code> from any images made in a single run with <code>--compilation-anim gif</code></li> <li>feature: create a series of images or edits by iterating over a parameter with the <code>--arg-schedule</code> argument</li> <li>feature: <code>openjourney-v1</code> and <code>openjourney-v2</code> models added. available via <code>--model openjourney-v2</code></li> <li>feature: add upscale command line function: <code>aimg upscale</code></li> <li>feature: <code>--gif</code> option will create a gif showing the generation process for a single image</li> <li>feature: <code>--compare-gif</code> option will create a comparison gif for any image edits</li> <li>fix: tile mode was broken since latest perf improvements</li> </ul> <p>8.2.0</p> <ul> <li>feature: added <code>aimg system-info</code> command to help debug issues</li> </ul> <p>8.1.0</p> <ul> <li>feature: some memory optimizations and documentation</li> <li>feature: surprise-me improvements</li> <li>feature: image sizes can now be multiples of 8 instead of 64. Inputs will be silently rounded down.</li> <li>feature: cleaned up <code>aimg</code> shell logs  </li> <li>feature: auto-regen for unsafe images</li> <li>fix: make blip filename windows compatible</li> <li>fix: make captioning work with alpha pngs</li> </ul> <p>8.0.5</p> <ul> <li>fix: bypass huggingface cache retrieval bug</li> </ul> <p>8.0.4</p> <ul> <li>fix: limit attention slice size on MacOS machines with 64gb (#175)</li> </ul> <p>8.0.3</p> <ul> <li>fix: use python 3.7 compatible lru_cache</li> <li>fix: use windows compatible filenames</li> </ul> <p>8.0.2</p> <ul> <li>fix: hf_hub_download() got an unexpected keyword argument 'token'</li> </ul> <p>8.0.1</p> <ul> <li>fix: spelling mistake of \"surprise\"</li> </ul> <p>8.0.0</p> <ul> <li>feature: \ud83c\udf89 edit images with instructions alone!</li> <li>feature: when editing an image add <code>--gif</code> to create a comparision gif</li> <li>feature: <code>aimg edit --surprise-me --gif my-image.jpg</code> for some fun pre-programmed edits</li> <li>feature: prune-ckpt command also removes the non-ema weights</li> </ul> <p>7.6.0</p> <ul> <li>fix: default model config was broken</li> <li>feature: print version with <code>--version</code></li> <li>feature: ability to load safetensors</li> <li>feature:  \ud83c\udf89 outpainting. Examples: <code>--outpaint up10,down300,left50,right50</code> or <code>--outpaint all100</code> or <code>--outpaint u100,d200,l300,r400</code></li> </ul> <p>7.4.3</p> <ul> <li>fix: handle old pytorch lightning imports with a graceful failure (fixes #161)</li> <li>fix: handle failed image generations better (fixes #83)</li> </ul> <p>7.4.2</p> <ul> <li>fix: run face enhancement on GPU for 10x speedup</li> </ul> <p>7.4.1</p> <ul> <li>fix: incorrect config files being used for non-1.0 models</li> </ul> <p>7.4.0</p> <ul> <li>feature: \ud83c\udf89 finetune your own image model. kind of like dreambooth. Read instructions on \"Concept Training\" page</li> <li>feature: image prep command. crops to face or other interesting parts of photo</li> <li>fix: back-compat for hf_hub_download</li> <li>feature: add prune-ckpt command</li> <li>feature: allow specification of model config file</li> </ul> <p>7.3.0</p> <ul> <li>feature: \ud83c\udf89 depth-based image-to-image generations (and inpainting) </li> <li>fix: k_euler_a produces more consistent images per seed (randomization respects the seed again)</li> </ul> <p>7.2.0</p> <ul> <li>feature: \ud83c\udf89 tile in a single dimension (\"x\" or \"y\").  This enables, with a bit of luck, generation of 360 VR images. Try this for example: <code>imagine --tile-x -w 1024 -h 512 \"360 degree equirectangular panorama photograph of the mountains\"  --upscale</code></li> </ul> <p>7.1.1</p> <ul> <li>fix: memory/speed regression introduced in 6.1.0</li> <li>fix: model switching now clears memory better, thus avoiding out of memory errors</li> </ul> <p>7.1.0</p> <ul> <li>feature: \ud83c\udf89 Stable Diffusion 2.1.  Generated people are no longer (completely) distorted.  Use with <code>--model SD-2.1</code> or <code>--model SD-2.0-v</code> </li> </ul> <p>7.0.0</p> <ul> <li>feature: negative prompting.  <code>--negative-prompt</code> or <code>ImaginePrompt(..., negative_prompt=\"ugly, deformed, extra arms, etc\")</code></li> <li>feature: a default negative prompt is added to all generations. Images in SD-2.0 don't look bad anymore. Images in 1.5 look improved as well.</li> </ul> <p>6.1.2</p> <ul> <li>fix: add back in memory-efficient algorithms</li> </ul> <p>6.1.1</p> <ul> <li>feature: xformers will be used if available (for faster generation)</li> <li>fix: version metadata was broken</li> </ul> <p>6.1.0</p> <ul> <li>feature: use different default steps and image sizes depending on sampler and model selected</li> <li>fix: #110 use proper version in image metadata</li> <li>refactor: solvers all have their own class that inherits from ImageSolver</li> <li>feature: \ud83c\udf89\ud83c\udf89\ud83c\udf89 Stable Diffusion 2.0</li> <li><code>--model SD-2.0</code> to use (it makes worse images than 1.5 though...) </li> <li>Tested on macOS and Linux</li> <li>All samplers working for new 512x512 model</li> <li>New inpainting model working</li> <li>768x768 model working for all samplers except PLMS (<code>--model SD-2.0-v</code>)</li> </ul> <p>5.1.0</p> <ul> <li>feature: add progress image callback</li> </ul> <p>5.0.1</p> <ul> <li>fix: support larger images on M1. Fixes #8</li> <li>fix: support CPU generation by disabling autocast on CPU. Fixes #81</li> </ul> <p>5.0.0</p> <ul> <li>feature: \ud83c\udf89 inpainting support using new inpainting model from RunwayML. It works really well! By default, the  inpainting model will automatically be used for any image-masking task </li> <li>feature: \ud83c\udf89 new default sampler makes image generation more than twice as fast</li> <li>feature: added <code>DPM++ 2S a</code> and <code>DPM++ 2M</code> samplers.</li> <li>feature: improve progress image logging</li> <li>fix: fix bug with <code>--show-work</code>. fixes #84</li> <li>fix: add workaround for pytorch bug affecting macOS users using the new <code>DPM++ 2S a</code> and <code>DPM++ 2M</code> samplers.</li> <li>fix: add workaround for pytorch mps bug affecting <code>k_dpm_fast</code> sampler. fixes #75</li> <li>fix: larger image sizes now work on macOS. fixes #8</li> </ul> <p>4.1.0</p> <ul> <li>feature: allow dynamic switching between models/weights <code>--model SD-1.5</code> or <code>--model SD-1.4</code> or <code>--model path/my-custom-weights.ckpt</code>)</li> <li>feature: log total progress when generating images (image X out of Y)</li> </ul> <p>4.0.0</p> <ul> <li>feature: stable diffusion 1.5 (slightly improved image quality)</li> <li>feature: dilation and erosion of masks  Previously the <code>+</code> and <code>-</code> characters in a mask (example: <code>face{+0.1}</code>) added to the grayscale value of any masked areas. This wasn't very useful. The new behavior is that the mask will expand or contract by the number of pixel specified. The technical terms for this are dilation and erosion.  This allows much greater control over the masked area.</li> <li>feature: update k-diffusion samplers. add k_dpm_adaptive and k_dpm_fast</li> <li>feature: img2img/inpainting supported on all samplers</li> <li>refactor: consolidates img2img/txt2img code. consolidates schedules. consolidates masking</li> <li>ci: minor logging improvements</li> </ul> <p>3.0.1</p> <ul> <li>fix: k-samplers were broken</li> </ul> <p>3.0.0</p> <ul> <li>feature: improved safety filter</li> </ul> <p>2.4.0</p> <ul> <li>\ud83c\udf89 feature: prompt expansion</li> <li>feature: make (blip) photo captions more descriptive</li> </ul> <p>2.3.1</p> <ul> <li>fix: face fidelity default was broken</li> </ul> <p>2.3.0</p> <ul> <li>feature: model weights file can be specified via <code>--model-weights-path</code> argument at the command line</li> <li>fix: set face fidelity default back to old value</li> <li>fix: handle small images without throwing exception. credit to @NiclasEriksen</li> <li>docs: add setuptools-rust as dependency for macos </li> </ul> <p>2.2.1</p> <ul> <li>fix: init image is fully ignored if init-image-strength = 0</li> </ul> <p>2.2.0</p> <ul> <li>feature: face enhancement fidelity is now configurable</li> </ul> <p>2.1.0</p> <ul> <li>improved masking accuracy from clipseg</li> </ul> <p>2.0.3</p> <ul> <li>fix memory leak in face enhancer</li> <li>fix blurry inpainting </li> <li>fix for pillow compatibility</li> </ul> <p>2.0.0</p> <ul> <li>\ud83c\udf89 fix: inpainted areas correlate with surrounding image, even at 100% generation strength.  Previously if the generation strength was high enough the generated image would be uncorrelated to the rest of the surrounding image.  It created terrible looking images.   </li> <li>\ud83c\udf89 feature: interactive prompt added. access by running <code>aimg</code></li> <li>\ud83c\udf89 feature: Specify advanced text based masks using boolean logic and strength modifiers. Mask descriptions must be lowercase. Keywords uppercase.    Valid symbols: <code>AND</code>, <code>OR</code>, <code>NOT</code>, <code>()</code>, and mask strength modifier <code>{+0.1}</code> where <code>+</code> can be any of <code>+ - * /</code>. Single character boolean operators also work (<code>|</code>, <code>&amp;</code>, <code>!</code>)</li> <li>\ud83c\udf89 feature: apply mask edits to original files with <code>mask_modify_original</code> (on by default)</li> <li>feature: auto-rotate images if exif data specifies to do so</li> <li>fix: mask boundaries are more accurate</li> <li>fix: accept mask images in command line</li> <li>fix: img2img algorithm was wrong and wouldn't at values close to 0 or 1</li> </ul> <p>1.6.2</p> <ul> <li>fix: another bfloat16 fix</li> </ul> <p>1.6.1</p> <ul> <li>fix: make sure image tensors come to the CPU as float32 so there aren't compatibility issues with non-bfloat16 cpus</li> </ul> <p>1.6.0</p> <ul> <li>fix: maybe address #13 with <code>expected scalar type BFloat16 but found Float</code></li> <li>at minimum one can specify <code>--precision full</code> now and that will probably fix the issue  </li> <li>feature: tile mode can now be specified per-prompt</li> </ul> <p>1.5.3</p> <ul> <li>fix: missing config file for describe feature</li> </ul> <p>1.5.1</p> <ul> <li>img2img now supported with PLMS (instead of just DDIM)</li> <li>added image captioning feature <code>aimg describe dog.jpg</code> =&gt; <code>a brown dog sitting on grass</code></li> <li>added new commandline tool <code>aimg</code> for additional image manipulation functionality</li> </ul> <p>1.4.0</p> <ul> <li>support multiple additive targets for masking with <code>|</code> symbol.  Example: \"fruit|stem|fruit stem\"</li> </ul> <p>1.3.0</p> <ul> <li>added prompt based image editing. Example: \"fruit =&gt; gold coins\"</li> <li>test coverage improved</li> </ul> <p>1.2.0</p> <ul> <li>allow urls as init-images</li> </ul> <p>previous</p> <ul> <li>img2img actually does # of steps you specify  </li> <li>performance optimizations</li> <li>numerous other changes</li> </ul>"},{"location":"ecosystem-notes/","title":"Ecosystem notes","text":""},{"location":"ecosystem-notes/#notable-stable-diffusion-implementations","title":"Notable Stable Diffusion Implementations","text":"<ul> <li>https://github.com/ahrm/UnstableFusion</li> <li>https://github.com/AUTOMATIC1111/stable-diffusion-webui</li> <li>https://github.com/blueturtleai/gimp-stable-diffusion</li> <li>https://github.com/hafriedlander/stable-diffusion-grpcserver</li> <li>https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/stable_diffusion</li> <li>https://github.com/lkwq007/stablediffusion-infinity</li> <li>https://github.com/lstein/stable-diffusion</li> <li>https://github.com/parlance-zz/g-diffuser-lib</li> <li>https://github.com/hafriedlander/idea2art</li> </ul>"},{"location":"ecosystem-notes/#image-generation-algorithms","title":"Image Generation Algorithms","text":"date name group use type FID params 2023-02-22 Composer [paper] [code] Alibaba Private 9.2 3.4B 2022-11-24 Stable Diffusion 2 StabilityAI Opensource 2022-09-28 Dall-E-2 OpenAI API 2022-08-22 Stable Diffusion StabilityAI Opensource - Latent Diffusion"},{"location":"ecosystem-notes/#further-reading","title":"Further Reading","text":"<ul> <li>https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</li> <li>Prompt Engineering Handbook</li> <li>Differences between samplers</li> <li>https://www.reddit.com/r/StableDiffusion/comments/xbeyw3/can_anyone_offer_a_little_guidance_on_the/</li> <li>https://www.reddit.com/r/bigsleep/comments/xb5cat/wiskkeys_lists_of_texttoimage_systems_and_related/</li> <li>https://huggingface.co/blog/annotated-diffusion</li> <li>https://github.com/jessevig/bertviz</li> <li>https://www.youtube.com/watch?v=5pIQFQZsNe8</li> <li>https://jalammar.github.io/illustrated-transformer/</li> <li>https://huggingface.co/blog/assets/78_annotated-diffusion/unet_architecture.jpg</li> </ul>"},{"location":"emad-qa-2020-10-10/","title":"Q&amp;A with Emad Mostaque","text":"<p>Lightly edited by Bryce Drennan, imaginAIry</p>"},{"location":"emad-qa-2020-10-10/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Summarized Version</li> <li>Full Version</li> <li>When will 1.5 be released?</li> <li>What are SD's views on artistic freedom versus censorship in models?  (3:24)</li> <li>Any update on the updated credit pricing model that was mentioned a couple of days ago, as in, is it getting much cheaper? (5:11)</li> <li>Can we get an official statement on why Automatic was banned (from the discord) and why NovelAI used his code? (5:53)</li> <li>Will Stability provide, fund, a model to create new medicines? (12:31)</li> <li>Do you think the new AI models push us closer to a post-copyright world? (14:04)</li> <li>Prompt engineering may well become an elective class in schools over the next decade. With extremely fast paced development, what do you foresee as the biggest barriers to entries? Some talking points might include a reluctance to adoption, death of the concept artist and the dangers outweighing the benefits. (15:27)</li> <li>How long does it usually take to train? (16:53)</li> <li>How close do you feel you might be able to show a full motion video model like Google or Meta showed up recently? (18:26)</li> <li>When do you think we will talk to an AI about the image? (18:35)</li> <li>How realistic do you think dynamically creating realistic 3d content with enough fidelity in a VR setting would be? what do you say the timeline on something like that is? (20:44)</li> <li>Any plans for stability to tackle open source alternatives to AI code generators, like copilot and alpha code? (25:01)</li> <li>Will support be added for inference at sizes other than 512 by default?</li> <li>Do you have any plans to improve the model in terms of face, limbs, and hand generation? Is it possible to improve on specifics on this checkpoint?</li> <li>I saw your partnership with AI Grant with Nat and Daniel. If you guys would support startups in case they aren't selected by them, any way startups can connect with you folks to get mentorship or guidance?</li> <li>Is Stability AI considering working on climate crisis via models in some way? (28:23)</li> <li>Which jobs do you think are most in danger of being taken by AI?</li> <li>What work is being done to improve the attention mechanism of stable diffusion to better handle and interpret composition while preserving artistic style? There are natural language limitations when it comes to interpreting physics from simple statements. Artistic style further deforms and challenges this kind of interpretation. Is stability AI working on high-level compositional language for use of generative models?</li> <li>What are the technical limitations around recreating SD with a 1024 dataset rather than 512, and why not have varying resolutions for the dataset? Is the new model going to be a ton bigger?</li> <li>Any plans for creating a worthy open source alternative, something like AI Dungeon or Character AI?</li> <li>When we'll be able to create full on movies with AI?</li> <li>Did you read the distillation of guided diffusion models paper? Do you have any thoughts on it? Like if it will improve things on consumer level hardware or just the high VRAM data centers?</li> <li>who do you think should own the copyright of an image video made by an AI or do you think there shouldn't be an owner?</li> <li>Update on adding more payment methods for dream studio?</li> <li>Are there any areas of the industry that is currently overlooked that you'll be excited to see the effects of diffusion based AI being used?</li> <li>Do you have any plans to release a speech since this model likes script overdone voices?</li> <li>Do you have any thoughts on increasing the awareness of generative models?  Is this something you see as important? How long do you think until the mass global population becomes aware of these models?</li> <li>Will we always be limited by the hardware cost to run AI or do you expect something to change?</li> <li>I'm unsure how to release licensed images based on SD output. Some suggest creative commons zero is fine.</li> <li>Is stability AI going to give commissions to artists?</li> <li>A text-to-speech model too?</li> <li>Is it possible to get vector images like an SVG file from stable diffusion or related systems?</li> <li>Is there a place to find all stable AI-made models in one place?</li> <li>Where do you see stability AI in five years?</li> <li>My question was about whether I have to pass down the rail license limitations when licensing SD based images or I can release as good. (45:06)</li> <li>As a composer and audio engineer myself, I cannot imagine AI will approach the emotional intricacies and depths of complexity found in music by world class musicians, at least not anytime soon. That said, I'm interested in AI as a tool, would love to explore how it can be used to help in this production process. Is stability AI involved in this?</li> <li>Are you guys working on LMs as well, something to compete with OpenAI GPT-3?</li> <li>In the future for other models, we are building an opt-in and opt-out system for artists and others that will lead to use in partnerships leading organizations. This model has some principles, the outputs are not direct for any single piece or initiatives of motion with regards to this.</li> <li>When will stability and EleutherAI be able to translate geese to speech in real time?</li> <li>Will dream studio front-end be open source so it can be used on local GPUs? (50:25)</li> <li>What do you think of the situation where a Google engineer believed the AI chatbot achieved sentience?</li> <li>Thoughts on getting seamless equirectangular 360 degree and 180 degree and HDR outputs in one shot for image to text and text to image.</li> <li>Any plans for text-to-3d diffusion models?</li> <li>With some of the recent backlash from artists, is there anything you wish that SD did differently in the earliest stages that would have changed the framing around image synthesis?</li> <li>Are you looking to decentralize GPU AI compute?</li> <li>Are we going to do nerf type models?</li> <li>Will AI lead to UBI? (56:09)</li> <li>When will we be able to generate games with AI?</li> <li>How's your faith influence your mission?</li> <li>How are you going to train speed cost and TPUs versus a one hundreds or the cost of switching TensorFlow from PyTorch?</li> <li>Does StabilityAI have plans to take on investors at any point or have they already?</li> <li>How much of an impact do you think AI will impact neural implant cybernetics? It appears one of the limiting facts of cybernetics is the input method, not necessarily the hardware. (1:00:24)</li> <li>Can you make cyberpunk 2077 not broken?</li> <li>Are you guys planning on creating any hardware devices? A more consumer-oriented one, which has AI as OS?</li> <li>Anything specific you'd like to see out of the community?</li> <li>How are you Emad?</li> <li>What's a good way to handle possible tribalism, extremism?</li> <li>Emad, real-life meetups for us members?</li> <li>Any collaboration in China yet? Can we use Chinese clip to guide the current one or do we need to retrain the model, embed the language clip into the model?</li> <li>Is there going to be a time when we have AI friends we create ourselves, personal companions speaking to us via our monitor, much of the same way a webcam call is done, high quality, et cetera?</li> <li>How early do you think we are in this AI wave that's emerging? With how fast it's changing it's hard not to feel FOMO. (1:10:28)</li> <li>Any comments on Harmony AI? How close do you think we are to having music sound AI with the same accessibility afforded by stable diffusion?</li> <li>When will you launch the full DreamStudio and will it be able to do animations? If so, do you think it'll be more cost effective than using Colab?</li> <li>Do you think an AI therapist could be something to address the lack of access to qualified mental health experts?</li> <li>What are your thoughts on Buckminster Fuller's work and his thoughts on how to build a world that doesn't destroy himself?</li> <li>How will generative models and unlimited custom tailored content to an audience of one impact how we value content? The paradox of choice is more options tend to make people more anxious and we get infinite choice right now. How do we get adapted to our new god-like powers in this hedonic treadmill? Is it a net positive for humanity? How much consideration are we given to potential bad outcomes?</li> <li>When do you think multi-models will emerge combining language, video and image?</li> <li>wrap up</li> </ul>"},{"location":"emad-qa-2020-10-10/#summarized-version","title":"Summarized Version","text":"<p>(This took too much effort so I've only summarized the first few questions)</p> <p>When will 1.5 be released?</p> <p>The developers have asked for more time before releasing this particular class of model, given some of the edge cases of danger here. The other part is the movement of the repository and the taking over from CompViz. this may seem like just hitting a fork button, but we've taken in legal counsel to make sure that we are doing the right thing and are fully protected. I believe that that process is nearly complete</p> <p>In the next couple of days we will be making three releases:  - the Discord bot will be open sourced  - there is a diffusion-based upscaler that is really quite snazzy that will be released as well  - a new decoder architecture for better human faces and other elements</p> <p>Our clip guidance instructions released soon that will enable you to have mid-journey level results.</p> <p>This particular class of models (image generation) needs to be released properly and responsibly, otherwise it's going to get very messy. Congresswoman Eshoo coming out and directly attacking us and asking us to be classified as dual-use technology and be banned by the NSA, there is European Parliament actions and others, because they just think the technology is too powerful.</p> <p>What are SD's views on artistic freedom versus censorship in models? (3:24)</p> <p>My view is basically if it's legal, then it should be allowed. The main thing that we want to try and do is that the model produces what you want it to produce, I think that's an important thing.</p> <p>Any update on the updated credit pricing model that was mentioned a couple of days ago, as in, is it getting much cheaper? (5:11)</p> <p>Yes, next week there'll be a credit pricing adjustment. You will be able to do a lot more with your credits, as opposed to the credits being changed in price.</p> <p>Can we get an official statement on why Automatic was banned (from the discord) and why NovelAI used his code? (5:53)</p> <p>Will Stability provide, fund, a model to create new medicines? (12:31)</p> <ul> <li>We're currently working on DNA diffusion that will be announced next week.</li> <li>LibreFold with Sergei Shrinikov's lab at Harvard and UCL, so that's probably going to be the most advanced protein folding model in the world, more advanced than AlphaFold.</li> </ul> <p>Do you think the new AI models push us closer to a post-copyright world? (14:04)</p> <p>I don't know, I think that's a very good question, it might. To be honest, no one knows what the copyright is around some of these things, like at what point does fair use stop and start and derivative works? It hasn't been tested, it will be tested,</p> <p>Prompt engineering may well become an elective class in schools over the next decade. With extremely fast paced development, what do you foresee as the biggest barriers to entries? Some talking points might include a reluctance to adoption, death of the concept artist and the dangers outweighing the benefits. (15:27)</p> <p>[I think prompt engineering won't really be a thing. The models will get better.]</p> <p>How long does it usually take to train? (16:53) - Stable Diffusion: 150,000 A100 hours at $4/hour on Amazon. 24 days on 256 A100s. - OpenClip: 1.2 Million hours</p> <p>How close do you feel you might be able to show a full motion video model like Google or Meta showed up recently? (18:26) We'll have it by the end of the year. But better.</p> <p>When do you think we will talk to an AI about the image? Like can you fix his nose a little bit or make a hair longer and stuff like that? (18:35)</p> <p>To be honest, I'm kind of disappointed in the community has not built that yet. All you have to do is whack whisper on the front end. Thank you, OpenAI.</p> <p>How do you feel about the use of generative technology being used by surveillance capitalists to further profit aimed goals? What kind of stability I do about this? (19:33)</p> <p>The only thing we can really do is offer alternatives. Out-compete.</p> <p>How realistic do you think dynamically creating realistic 3d content with enough fidelity in a VR setting would be? what do you say the timeline on something like that is? (20:44)</p> <p>It's going to come within four to five years, fully high res, 2k in each eye resolution via even 4k or 8k actually, it just needs an M2 chip with the specialist transformer architecture in there. </p> <p>We have a ton of partnerships that we'll be announcing over the next few months, where we're converting closed source AI companies into open source AI companies.</p> <p>What guarantees does the community have that stability AI won't go down on the same path as OpenAI? That one day you won't develop a good enough model, you decide to close things after benefiting from all the work of the community and the visibility generated by it? (22:55)</p> <p>That's a good question. I mean, it kind of sucks what happened with open AI, right? The R&amp;D team and the developers have in their contracts that they can release any model that they work on open source. So legally, we can't stop them.</p> <p>Any plans for stability to tackle open source alternatives to AI code generators, like copilot and alpha code? (25:01)</p> <p>Yeah, you can go over to carper.ai, and see our code generation model that's training right now.</p>"},{"location":"emad-qa-2020-10-10/#full-version","title":"Full Version","text":"<p>(lightly edited)</p> <p>When will 1.5 be released?</p> <p>1.5 isn't that big an improvement over 1.4, but it's still an improvement. And as we go into version 3 and the Imagen models that are training away now, which is like we have a 4.3 billion parameter one and others, we're considering what is the best data for that. What's the best system for that to avoid extreme edge cases, because there's always people who want to spoil the party? This has caused the developers themselves, and again, kind of I haven't done a big push here, it has been from the developers, to ask for a bit more time to consult and come up with a proper roadmap for releasing this particular class of model. They will be released for research and other purposes, and again, I don't think the license is going to change from the open rail end license, it's just that they want to make sure that all the boxes are ticked rather than rushing them out, given, you know, some of these edge cases of danger here.</p> <p>The other part is the movement of the repository and the taking over from CompViz, which is an academic research lab, again, who had full independence, relatively speaking, over the creation of decisions around the model, to StabilityAI itself.</p> <p>Now this may seem like just hitting a fork button, but you know, we've taken in legal counsel and a whole bunch of other things, just making sure that we are doing the right thing and are fully protected around releasing some of these models in this way. I believe that that process is nearly complete, it certainly cost us a lot of money, but you know, it will either be ourselves or an independent charity maintaining that particular repository and releasing more of these generative models.</p> <p>Stability itself, and again, kind of our associated entities, have been releasing over half a dozen models in the last weeks, so a model a week effectively, and in the next couple of days we will be making three releases, so the Discord bot will be open sourced, there is a diffusion-based upscaler that is really quite snazzy that will be released as well, and then finally there will be a new decoder architecture that Rivers Have Wings has been working on for better human faces and other elements trained on the aesthetic and humans thing. The core models themselves are still a little bit longer while we sort out some of these edge cases, but once that's in place, hopefully we should be able to release them as fast as our other models, such as for example the open clip model that we released, and there will be our clip guidance instructions released soon that will enable you to have mid-journey level results utilising those two, which took 1.2 million A100 hours, so like almost eight times as much as stable diffusion itself.</p> <p>Similarly, we released our language models and other things, and those are pretty straightforward, they are MIT, it's just again, this particular class of models needs to be released properly and responsibly, otherwise it's going to get very messy. Some of you will have seen a kind of Congresswoman Eshoo coming out and directly attacking us and asking us to be classified as dual-use technology and be banned by the NSA, there is European Parliament actions and others, because they just think the technology is too powerful., we are working hard to avoid that, and again, we'll continue from there.</p> <p>What are SD's views on artistic freedom versus censorship in models?  (3:24)</p> <p>My view is basically if it's legal, then it should be allowed, if it's illegal, then we should at least take some steps to try and adjust things around that, now that's obviously a very complicated thing, as legal is different in a lot of different countries, but there are certain things that you can look up the law, that's illegal to create anywhere. I'm in favour of more permissiveness, and you know, leaving it up to localised ethics and morality, because the reality is that that varies dramatically across many years, and I think it's our place to kind of police that, similarly, as you've seen with Dream Booth and all these other extensions on stable diffusion, these models are actually quite easy to train, so if something's not in the dataset, you can train it back in, if it doesn't fit in with the legal area of where we ourselves release from.</p> <p>So I think, you know, again, what's legal is legal, ethical varies, et cetera, the main thing that we want to try and do is that model produces what you want it to produce, I think that's an important thing.</p> <p>I think you guys saw at the start, before we had all the filters in place, that stable diffusion trained on the snapshot of the internet, as it was, it's just, when you type to the women, it had kind of toplessness for a lot of any type of artistic thing, because a lot of topless women in art, even though art is less than like, 4.5% of the dataset, you know, that's not what people wanted, and again, we're trying to make it so that it produces what you want, as long as it is legal, I think that's probably the core thing here.</p> <p>Any update on the updated credit pricing model that was mentioned a couple of days ago, as in, is it getting much cheaper? (5:11)</p> <p>Yes, next week, there'll be a credit pricing, a credit pricing adjustment from our side. There have been lots of innovations around inference and a whole bunch of other things, and the team has been testing it in staging and hosting. You've seen this as well in the diffusers library and other things, Facebook recently came out with some really interesting fast attention kind of elements, and we'll be passing on all of those savings. The way that it'll probably be is that credits will remain as is, but you will be able to do a lot more with your credits, as opposed to the credits being changed in price, because I don't think that's fair to anyone if we change the price of the credits.</p> <p>Can we get an official statement on why Automatic was banned (from the discord) and why NovelAI used his code? (5:53)</p> <p>I don't particularly like discussing individual user bans and things like that, but this was escalated to me because it's a very special case, and it comes at a time, again, of increased notice on the community and a lot of these other things. I've been working very hard around this. Automatic created a wonderful web UI that increased the accessibility of stable diffusion to a lot of different people. You can see that by the styles and other things. It's not open source, and I believe there is a copyright on it, but still, again, work super hard. A lot of people kind of helped out with that, and it was great to see. However, we do have a very particular stance on community as to what's acceptable and what's not.</p> <p>I think it's important to kind of first take a step back and understand what stability is and what stable diffusion is and what this community is, right? AI is a company that's trying to do good. We don't have profit as our main thing. We are completely independent. It does come a lot from me and me trying to do my best as I try to figure out governance structures to fit things, but I do listen to the devs. I do listen to my team members and other things. Obviously, we have a profit model and all of that, but to be honest, we don't really care about making revenue at the moment because it's more about the deep tech that we do. We don't just do image. We do protein folding. We release language models, code models, the whole gamut of things. In fact, we are the only multimodal AI company other than OpenAI, and we release just about everything with the exception of generative models until we figure out the processes for doing that. MIT open-sourced. What does that mean? It means that literally everything is open-sourced. Against that, we come under attack. So our model weights, when we released it for academia, were leaked. We collaborate with a lot of entities, so NovelAI is one of them, and their engineers have hit with various code-based things, and I think we've helped as well. They are very talented engineers, and you'll see they've just released a list of all the things that they did to improve stable diffusion because they were actually going to open-source it very soon, I believe it was next week, before the code was stolen from their system. We have a very strict no-support policy for stolen code because this is a very sensitive area for us. We do not have a commercial partnership with NovelAI. We do not pay them. They do not pay us. They're just members of the community like any other, but when you see these things, if someone stole our code and released it and it was dangerous, I wouldn't find that right. If someone stole their code, if someone stole other codes, I don't believe that's right either in terms of releasing.</p> <p>Now in this particular case, what happened is that the community member and person was contacted and there was a conversation made. He made some messages public. Other messages were not made public. I looked at all the facts. I decided that this was a banable offense on the community. I'm not a stupid person. I am technical. I do understand a lot of things, and I put everyone there to kind of make this as a clear point. Stable diffusion community itself is one of community of stability AI, and it's one community of stable diffusion. Stable diffusion is a model that's available to the whole world, and you can build your own communities and take this in a million different ways. It is not healthy if stability AI is at the center of everything that we do, and that's not what we're trying to create. We're trying to create a multiplicity of different areas that you can discuss and take things forward and communities that you feel you yourself are a stable part of. Now, this particular one is regulated, and it is not a free-for-all. It does have specific rules, and there are specific things within it. Again, it doesn't mean that you can't go elsewhere to have these discussions. We didn't take it down off GitHub or things like that. We leave it up to them, but the manner in which this was done and there are other things that aren't made public, I did not feel it was appropriate, and so I approved the banning and the buck stops with me there.</p> <p>If the individual in question wants to be unbanned and rejoin the community, there is a process for repealing bans. We have not received anything on that side, and I'd be willing to hear other stuff if maybe I didn't have the full picture, but as it is, that's where it stands, and again, like I said, we cannot support any illegal theft as direct theft in there. With regards to the specific code point, you can ask novel AI themselves what happened there. They said that there was AGPL code copied over, and then they rescinded it as soon as it was notified, and they apologized. That did not happen in this case, and again, we cannot support any leaked models, and we cannot support that because, again, the safety issues around this and the fact that if you start using leaked and stolen code, there are some very dangerous liability concerns that we wish to protect the community from. We cannot support that particular code base at the moment, and we can't support that individual being a member of the community.</p> <p>Also, I would like to say that a lot of insulting things were said, and we let it slide this once. Don't be mean, man. Just talk responsibly. Again, we're happy to have considered and thought-out discussions offline and online. If you do start insulting other members, then please flag it to moderators, and there will be timeouts and bans because, again, what is this community meant to be? It's meant to be quite a broad but core and stable community that is our private community as Stability AI, but, like I said, the beauty of open source is that if this is not a community you're comfortable with, you can go to other communities. You can set up your own communities. You can set up your notebooks and others. In fact, when you look at it, just about every single web UI has a member of Stability contributing. From Pharma Psychotic at DeForum through to Dango on Majesty through to Gandamu at Disco, we have been trying to push open source front-ends with no real expectations of our own because we believe in the ability for people to remix and build their own communities around that. Stability has no presence in these other communities because those are not our communities. This one is.</p> <p>So, again, like I said, if Automattic does want to have a discussion, my inbox is open, and if anyone feels that they're unjustly timed out or banned, they can appeal them. Again, there is a process for that. That hasn't happened in this case, and, again, it's a call that I made looking at some publicly available information and some non-publicly available information, and I wish them all the best.</p> <p>Will Stability provide, fund, a model to create new medicines? (12:31)</p> <p>We're currently working on DNA diffusion that will be announced next week for some of the DNA expression things in our openBioML community. Feel free to join that. It's about two and a half thousand members, and currently I believe it's been announced LibreFold with Sergei Shrinikov's lab at Harvard and UCL, so that's probably going to be the most advanced protein folding model in the world, more advanced than AlphaFold. It's just currently undergoing ablations. Repurposing of medicines and discovery of new medicines is something that's very close to my heart. Many of you may know that basically the origins of Stability were leading and architecting and running the United Nations AI Initiative against COVID-19, so I was the lead architect of that to try and get a lot of this knowledge coordinated around that. We made all the COVID research in the world free and then helped organize it with the backing of the UNESCO World Bank and others, so that's one of the genesis' alongside education. For myself as well, if you listen to some of my podcasts, I quit being a hedge fund manager for five years to work on repurposing drugs for my son, doing AI-based lit review and repurposing of drugs through neurotransmitter analysis. So taking things like nazepam and others to treat the symptoms of ASD, the papers around that will be published and we have several initiatives in that area, again, to try and just catalyze it going forward, because that's all we are, we're a catalyst. Communities should take up what we do and run forward with that.</p> <p>Do you think the new AI models push us closer to a post-copyright world? (14:04)</p> <p>I don't know, I think that's a very good question, it might. To be honest, no one knows what the copyright is around some of these things, like at what point does free use stop and start and derivative works? It hasn't been tested, it will be tested, I'm pretty sure there will be all sorts of lawsuits and other things soon, again, that's something we're preparing for. But I think one of the first AI pieces of art was recently granted a copyright. I think the ability to create anything is an interesting one as well, because again, it makes content more valuable, so in an abundance scarcity is there, but I'm not exactly sure how this will play out. I do think you'll be able to create anything you want for yourselves, it just becomes, what happens when you put that into a social context and start selling that? This comes down to the personal agency side of the models that we build as well, you know, like you're responsible for the inputs and the outputs that result from that. And so this is where I think copyright law will be tested the most, because people usually did not have the means of creation, whereas now you have literally the means of creation.</p> <p>Prompt engineering may well become an elective class in schools over the next decade. With extremely fast paced development, what do you foresee as the biggest barriers to entries? Some talking points might include a reluctance to adoption, death of the concept artist and the dangers outweighing the benefits. (15:27)</p> <p>Well, you know, the interesting thing here is that a large part of life is the ability to prompt. So, you know, prompting humans is kind of the key thing, like my wife tries to prompt me all the time, and she's not very successful, but she's been working on it for 16 years. I think that a lot of the technologies that you're seeing right now from AI, because it understands these latent spaces or hidden meanings, it also includes the hidden meanings in prompts, and I think what you see is you have these generalized models like stable diffusion and stable video fusion and dance diffusion and all these other things. It pushes intelligence to the edge, but what you've done is you compressed 100,000 gigabytes of images into a two gigabyte file of knowledge that understands all those contextualities. The next step is adapting that to your local context. So that's what you guys do when you use Dreambooth, or when you do textual inversion, you're injecting a bit yourself into that model so it understands your prompts better. And I think a combination of multiple models doing that will mean that prompt engineering isn't really the thing, it's just understanding how to chain these tools together, so more kind of context specific stuff. This is why we're partnered with an example for Replit, so that people can build dynamic systems and we've got some very interesting things on the way there. I think the barriers to entry will drop dramatically, like do you really need a class on that? For the next few years, yeah, but then soon it will not require that.</p> <p>How long does it usually take to train? (16:53)</p> <p>Well, that's a piece of string. It depends. We have models, so stable diffusion of 150,000 A100 hours, and A100 hours about $4 on Amazon, which you need for the interconnect. Open clip was 1.2 million hours. That's literally hours of compute. So for stable diffusion, can someone in the chat do this? It's 256 A100s over 150,000 hours. So divide one by the other. What's the number? Let me get it quick. Quickest. Ammonite? Ammonite, you guys kind of calculate slow. 24 days, says Ninjaside. There we go. That's about how long it took to train the model. To do the tests and other stuff, it took a lot longer. And the bigger models, again, it depends because it doesn't really need any scale. So it's not that you chuck 512 and it's more efficient. It is really a lot of the heavy lifting is done by the super compute. So what happens is that we're doing all this work up front, and then we release the model to everyone. And then as Joe said, DreamBooth takes about 15 minutes on an A100 to then fine tune. Because all the work of those years of knowledge, the thousands of gigabytes, are all done for you. And that's why you can take it and extend it and kind of do what you want with it. That's the beauty of this model over the old school internet, which was always computing all the time. So you can push intelligence to the edges. All right.</p> <p>How close do you feel you might be able to show a full motion video model like Google or Meta showed up recently? (18:26)</p> <p>We'll have it by the end of the year. But better.</p> <p>When do you think we will talk to an AI about the image? (18:35)</p> <p>Like can you fix his nose a little bit or make a hair longer and stuff like that? To be honest, I'm kind of disappointed in the community has not built that yet. It's not complicated. All you have to do is whack whisper on the front end. Thank you, OpenAI. You know, obviously, you know, that was a great benefit and then have that input into style clip or a kind of fit based thing. So if you look up, Max Wolf has this wonderful thing on style clip that you can see how to create various scary Zuckerberg's as if he wasn't scary himself. And so I'm putting that into the pipeline basically allows you to do what it says there with a bit of targeting. So there's some star clip right there in the stage chat. And again, with the new clip models that we have and a bunch of the other bit models that Google have released recently, you should be able to do that literally now when you can buy that with whisper.</p> <p>How do you feel about the use of generative technology being used by surveillance capitalists to further profit aimed goals? What kind of stability I do about this? (19:33)</p> <p>The only thing we can really do is offer alternatives like do you really want to be in a meta what do they call it, horizon first where you got no legs or genitals, not really, you know, like legs are good, genitals good. And so by providing open alternatives, we can basically out compete the rest like look at the amount of innovation that's happened on the back of stable diffusion. And again, you know, acknowledge our place in that we don't police it, we don't control it, you know, like people can take it and extend it. If you want to use our services, great. If you don't, it's fine. We're creating a brand new ecosystem that will out compete the legacy guys, because thousands millions of people will be building and developing on this. Like we are sponsoring the faster AI course on stable diffusion, so that anyone who's a developer can rapidly learn to be a stable diffusion developer. And you know, this isn't just kind of interfaces and things like that. It's actually you'll be able to build your own models. And how crazy is that? Let's make it accessible to everyone and again, that's why we're working with gradios and others on that.</p> <p>How realistic do you think dynamically creating realistic 3d content with enough fidelity in a VR setting would be? what do you say the timeline on something like that is? (20:44) You know, unless you're Elon Musk, self driving cars have always been five years away. Always always, you know, $100 billion has been spent on self driving cars, and the research and it's to me, it's not that much closer. The dream of photorealistic VR though is very different with generative AI. Like again, look at the 24 frames per second image and video look at the long fanaki video as well and then consider Unreal Engine 5 what's Unreal Engine 6 going to look like? Well, it'll be photorealistic right and it'll be powered by nerf technology. The same as Apple is pioneering for use on the neural engine chips that make up 16.8% of your MacBook M1 GPU. It's going to come within four to five years, fully high res, 2k in each eye resolution via even 4k or 8k actually, it just needs an M2 chip with the specialist transformer architecture in there. And that will be available to a lot of people. But then like I said, Unreal Engine 6 will also be out in about four or five years. And so that will also up the ante. There's a lot of amazing compression and customized stuff you can do around this. And so I think it's just gonna be insane when you can create entire worlds. And hopefully, it'll be built on the type of architectures that we help catalyze, whether it's built by ourselves or others. So we have a metric shit ton, I believe is the appropriate term of partnerships that we'll be announcing over the next few months, where we're converting closed source AI companies into open source AI companies, because, you know, it's better to work together. And again, we shouldn't be at the center of all this with everything laying on our shoulders. But it should be a teamwork initiative, because this is cool technology that will help a lot of people.</p> <p>What guarantees does the community have that stability AI won't go down on the same path as OpenAI? That one day you won't develop a good enough model, you decide to close things after benefiting from all the work of the community and the visibility generated by it? (22:55)</p> <p>That's a good question. I mean, it kind of sucks what happened with open AI, right? You can say it's safety, you can say it's commercials, like whatever. The R&amp;D team and the developers have in their contracts, except for one person that we need to send it to, that they can release any model that they work on open source. So legally, we can't stop them. Well, I think that's a pretty good kind of thing. I don't think there's any company in the world that does that. And again, if you look at it, the only thing that we haven't instantly released is this particular class of generative models, because it's not straightforward. And because you have frickin Congresswoman petitioning to ban us by the NSA. And a lot more stuff behind that. Look, you know, we're gonna get B Corp status soon, which puts in our official documents that we are mission focused, not profit focused. At the same time, I'm going to build $100 billion company that helps a billion people. We have some other things around governance that we'll be introducing as well. But currently, the governance structure is simple, yet not ideal, which is that I personally have control of board, ordinary common everything. And so a lot is resting on my shoulders are not sustainable. As soon as we figure that out, and how to maintain the independence and how to maintain it so that we are dedicated to open, which I think is a superior business model, a lot of people agree with, will implement that posthaste any suggestions, please do send them our way. But like I said, one core thing is, if we stop being open source, and go down the open AI route, there's nothing we can do to stop the developers from releasing the code. And without developers, what are we, you know, nice front end company that does a bit of model deployment, though it'd be killing ourselves.</p> <p>Any plans for stability to tackle open source alternatives to AI code generators, like copilot and alpha code? (25:01)</p> <p>Yeah, you can go over to carper.ai, and see our code generation model that's training right now. We released one of the FID based language models that will be core to that plus our instruct framework, so that you can have the ideal complement to that. So I think by Q1 of next year, we will have better code models than copilot. And there's some very interesting things in the works there, you just look at our partners and other things. And again, there'll be open source available to everyone.</p> <p>Will support be added for inference at sizes other than 512 by default?</p> <p>Yeah, I mean, there are kind of things like that already. So like, if you look at the recently released novel AI improvements to stable diffusion, you'll see that there are details there as to how to implement arbitrary resolutions similar to something like mid journey, I'll just post it there. The model itself, like I said, enables that it's just that the kind of code wasn't there. It was part of our expected upgrades. And again, like different models have been trained at different sizes. So we have a 768 model, a 512 model, et cetera, so 1024 model, et cetera, coming in the pipeline. I mean, like, again, I think that not many people have actually tried to train models yet. You can get into grips with it, but you can train and extend this, again, view it as a base of knowledge onto which you can adjust a bunch of other stuff.</p> <p>Do you have any plans to improve the model in terms of face, limbs, and hand generation? Is it possible to improve on specifics on this checkpoint?</p> <p>Yep, 100%. So I think in the next day or so, we'll be releasing a new fine-tuned decoder that's just a drop-in for any latent diffusion or stable diffusion model that is fine-tuned on the face-lion dataset, and that makes better faces. Then, as well, you can train it on, like, Hagrid, which is the hand dataset to create better hands, et cetera. Some of this architecture is known as a VAE architecture for doing that. And again, that's discussed a bit in the novel AI thing, because they do have better hands. And again, this knowledge will proliferate around that.</p> <p>I saw your partnership with AI Grant with Nat and Daniel. If you guys would support startups in case they aren't selected by them, any way startups can connect with you folks to get mentorship or guidance?</p> <p>We are building a grant program and more. It's just that we're currently hiring people to come and run it. That's the same as Bruce.Codes' question. In the next couple of weeks, there will be competitions and all sorts of grants announced to kind of stimulate the growth of some essential parts of infrastructure in the community. And we're going to try and get more community involvement in that, so people who do great things for the community are appropriately awarded. There's a lot of work being done there.</p> <p>Is Stability AI considering working on climate crisis via models in some way? (28:23)</p> <p>Yes, and this will be announced in November. I can't announce it just yet. They want to do a big, grand thing, but you know. We're doing that. We're supporting several entities that are doing climate forecasting functions and working with a few governments on weather patterns using transformer-based technologies as well.</p> <p>Which jobs do you think are most in danger of being taken by AI?</p> <p>I don't know, man. It's a complex one. I think that probably the most dangerous ones are call center workers and anything that involves human-to-human interaction. I don't know if you guys have tried character.ai. I don't know if they've stopped it because you could create some questionable entities. The... It's very good. And it will just get better because I think you look at some of the voice models we have coming up, you can basically do emotionally accurate voices and all sorts of stuff and voice-to-voice, so you won't notice a call center worker. But that goes to a lot of different things. I think that's probably the first for disruption before anything else. I don't think that artists get disrupted that much, to be honest, by what's going on here. Unless you're a bad artist, in which case you can use this technology to become a great artist, and the great artist will become even greater. So I think that's probably my take on that.</p> <p>What work is being done to improve the attention mechanism of stable diffusion to better handle and interpret composition while preserving artistic style? There are natural language limitations when it comes to interpreting physics from simple statements. Artistic style further deforms and challenges this kind of interpretation. Is stability AI working on high-level compositional language for use of generative models?</p> <p>The answer is yes. This is why we spent millions of dollars releasing the new CLIP. CLIP is at the core of these models. There's a generative component and there is a guidance component, and when you infuse the two together, you get models like they are right now. The guidance component, we used CLIP-L, which was CLIP-Large, which was the largest one that OpenAI released. They had two more, H and G, which I believe are huge and gigantic. We released H in the first version of G, which should take like a million A100 hours to do, and that improves compositional qualities so that as that gets integrated into a new version of stable diffusion, it will be at the level of DALY2, just even with a small size.</p> <p>There are some problems around this in that the model learns from both things. It learns from the stuff the generative thing is fine-tuned on and from the CLIP models, and so we've been spending a lot of time over the last few weeks, and there's another reason for the delay, seeing what exactly does this thing know, because even if an artist isn't in our training dataset, it somehow knows about it, and it turns out it was CLIP all along. So we really wanted to output what we think it outputs and not output what it shouldn't output, so we've been doing a lot of work around that. Similarly, what we found is that embedding pure language models like T5, XXL, and we tried UL2 and some of these other models, these are like pure language models like GPT-3, improves the understanding of these models, which is kind of crazy. And so there's some work being done around that for compositional accuracy, and again, you can look at the blog by Novel.ai where they extended the context window so that it can accept three times the amount of input from this.</p> <p>So your prompts get longer from I think like 74 to 225 or something like that, and there are various things you can do once you do proper latence place exploration, which I think is probably another month away, to really hone down on this. I think again, a lot of these other interfaces from the ones that we support to others have already introduced negative prompting and all sorts of other stuff. You should have kind of some vector-based initialization, et cetera, coming soon.</p> <p>What are the technical limitations around recreating SD with a 1024 dataset rather than 512, and why not have varying resolutions for the dataset? Is the new model going to be a ton bigger?</p> <p>So version 3 right now has 1.4 billion parameters. We've got a 4.3 billion parameter image in training and 900 million parameter image in training. We've got a lot of models training. We're just waiting to get these things right before we just start releasing them one after the other. The main limitation is the lack of 1024 images in the training dataset. Like Lion doesn't have a lot of high resolution images, and this is one of the things why what we've been working on the last few weeks is to basically negotiate and license amazing datasets that we can then put out to the world so that you can have much better models. And we're going to pay a crap load for that, but again, release it for free and open source to everyone. And I think that should do well. This is also why the upscaler that you're going to see is a two times upscaler. That's good. Four times upscaling is a bit difficult for us to do. Like it's still decent because we're just waiting on the licensing of those images.</p> <p>Any plans for creating a worthy open source alternative, something like AI Dungeon or Character AI?</p> <p>Well, a lot of the Carper AI teams work around instruct models and contrastive learning should enable Carper Character AI type systems on chatbots. And you know, from narrative construction to others, again, it will be ideal there. The open source versions of Novel AI and AI Dungeon, I believe the leading one is Cobold AI. So you might want to check that out. I haven't seen what the case has been with that recently.</p> <p>When we'll be able to create full on movies with AI?</p> <p>I don't know, like five years again.</p> <p>I'm just digging that out there. Okay, if I was Elon Musk, I'd say one year. I mean, it depends what you mean by a feature like movies. So like animated movies, when you combine stable diffusion with some of the language models and some of the code models, you should be able to create those.</p> <p>Maybe not in a UFO table or Studio Bones style within two years, I'd say, but I'd say a five year time frame for being able to create those in high quality, like super high res is reasonable because that's the time it will take to create these high res dynamic VR kind of things. To create fully photorealistic proper people movies, I mean, you can look at E.B. Synth or some of these other kind of pathway analyses, it shouldn't be that long to be honest. It depends on how much budget and how quick you want to do it.</p> <p>Real time is difficult, but you're going to see some really amazing real time stuff in the next year. Touch wood. We're lining it up. It's going to blow everyone's socks away. That's going to require a freaking supercomputer, but it's not movie length. It's something a bit different.</p> <p>Did you read the distillation of guided diffusion models paper? Do you have any thoughts on it? Like if it will improve things on consumer level hardware or just the high VRAM data centers?</p> <p>I mean, distillation and instructing these models is awesome. And the step counts they have for kind of reaching cohesion are kind of crazy. RiversideWigs has done a lot of work on a kind of DDPM fast solvent, but already reduced the number of steps required to get to those stages.</p> <p>And again, like I keep telling everyone, once you start chaining these models together, you're going to get down really sub one second and further, because I think you guys have seen image to image work so much better if you just even give a basic sketch than text to image.</p> <p>So why don't you chain together different models, different modalities to kind of get them? And I think it'll be easier once we release our various model resolution sizes plus upscalers so you can dynamically switch between models. If you look at the dream studio kind of teaser that I posted six weeks ago, that's why we've got model chaining integrated right in there.</p> <p>who do you think should own the copyright of an image video made by an AI or do you think there shouldn't be an owner?</p> <p>I think that if it isn't based on copyrighted content, it should be owned by the prompter of the AI. If the AI is a public model and not owned by someone else, otherwise it is almost like a code creation type of thing. But I'm not a lawyer and I think this will be tested severely very soon.</p> <p>Update on adding more payment methods for dream studio?</p> <p>I think we'll be introducing some alternate ones soon, the one that we won't introduce is PayPal. No, no PayPal, because that's just crazy what's going on there.</p> <p>With stable diffusion having been publicly released for over a month now and with the release of version five around the corner, what is the most impressive implementation you've seen someone create out of the application so far?</p> <p>I really love the dream booth stuff. I mean, come on, that shit's crazy. You know, even though some of you fine tuned me into kind of weird poses. I think it was pretty good. I didn't think we would get that level of quality. I thought it would be a textual and version level quality. Beyond that, I think that, you know, there's been this well of creativity, like you're starting to see some of the 3D stuff come out and again, I didn't think we'd get quite there even with the chaining. I think that's pretty darn impressive.</p> <p>Are there any areas of the industry that is currently overlooked that you'll be excited to see the effects of diffusion based AI being used?</p> <p>Again, like I can't get away from this PowerPoint thing. Like it's such a straightforward thing that causes so much real annoyance. I think we could kind of get it out there. I think it just requires kind of a few fine tuned models plus a code model plus a language model to kind of kick it together. I mean, diffusion is all about de-noising and information is about noise. So our brains filter out noise and de-noise all the time. So these models can be used in a ridiculous number of scenarios. Like I said, we've got DNA diffusion model going on in OpenBIM, all that shit crazy, right? But I think right now I really want to see some of these practical high impact use cases like the PowerPoint kind of thing.</p> <p>Do you have any plans to release a speech since this model likes script overdone voices?</p> <p>Yes, we have a plan to release a speech to speech model soon and some other ones around that. I think AudioLM by Google was super interesting recently. For those who don't know, that's basically you give it a snippet of a voice or of music or something and it just extends it. It's kind of crazy. But I think we get the arbitrary kind of length thing there and combined with some other models that could be really interesting.</p> <p>Do you have any thoughts on increasing the awareness of generative models?  Is this something you see as important? How long do you think until the mass global population becomes aware of these models?</p> <p>I think I can't keep up as it is and I don't want to die. But more realistically, we have a B2B2C model. So we're partnering with the leading brands in the world and content creators to both get their content so we can build better open models and to get this technology out to just everyone. Similar on a country basis, we have country level models coming out very soon. So on the language side of things, you can see we released Polyglot, which is the best Korean language model, for example, Vera, Luther AI and our support of them recently. So I think you will see a lot of models coming soon, a lot of different kind of elements around that.</p> <p>Will we always be limited by the hardware cost to run AI or do you expect something to change?</p> <p>Yeah, I mean, like this will run on the edge, it'll run on your iPhone in a year. Stable diffusion will run on an iPhone in probably seconds, that level of quality. That's again, a bit crazy.</p> <p>I'm unsure how to release licensed images based on SD output. Some suggest creative commons zero is fine.</p> <p>Okay, so if someone takes a CCO out image and violates the license, then something can be done around that. I would suggest that if you're worried about some of this stuff, you, CCO licensing, and again, I am not a lawyer, please consult with a lawyer, does not preclude copyright. And there's a transformational element that incorporates that. If you look at artists like Necro 13 and Claire Selva and others, you will see that the outputs usually aren't one shot, they are multi-sesic. And then that means that this becomes one part of that, a CCO license part that's part of your process. Like, even if you use GFPGAN or upscaling or something like that, again, I'm not a lawyer, please consult with one. I think that should be sufficiently transformative that you can assert full copyright over the output of your work.</p> <p>Is stability AI going to give commissions to artists?</p> <p>We have some very exciting in-house artists coming online soon. Some very interesting ones, I'm afraid that's all I can say right now. But yeah, we will have more art programs and things like that as part of our community engagement. It's just that right now it's been a struggle even to keep Discord and other things going and growing the team. Like, we're just over a hundred people now, God knows how many we actually need. I think we probably need to hire another hundred more.</p> <p>A text-to-speech model too?</p> <p>Yep. I couldn't release it just yet as my sister-in-law was running Synantic, but now that she's been absorbed by Spotify, we can release emotional text-to-speech. Not soon though, I think that we want to do some extra work around that and build that up.</p> <p>Is it possible to get vector images like an SVG file from stable diffusion or related systems?</p> <p>Not at the moment. You can actually do that with a language model, as you'll find out probably in the next month. But right now I would say just use a converter, and that's probably going to be the best way to do that.</p> <p>Is there a place to find all stable AI-made models in one place?</p> <p>No, there is not, because we are disorganized. We barely have a careers page up, and we're not really keeping a track of everything. We are employing someone as an AI librarian to come and help coordinate the community and some of these other things. Again, that's just a one-stop shop there. But yeah, also there's this collaborative thing where we're involved in a lot of stuff. There's a blurring line between what we need and what we don't need. We just are going to want to be the catalyst for all of this. I think the best models go viral anyway.</p> <p>Where do you see stability AI in five years?</p> <p>Hopefully with someone else leading the damn thing so I can finish Elden Ring.</p> <p>No, I mean, our aim is basically to build AI subsidiaries in every single country so that there's localized models for every country and race that are all open and to basically be the biggest, best company in the world that's actually aligned with you rather than trying to suck up your attention to serve you ads.</p> <p>I really don't like ads, honestly, unless they're artistic, I like artistic ads. So the aim is to build a big company to list and to give it back to the people so ultimately it's all owned by the people.</p> <p>For myself, my main aim is to ramp this up and spread as much profit as possible into Imagine Worldwide, our education arm run by our co-founder, which currently is teaching kids literacy and numeracy in refugee camps in 13 months on one hour a day. We've just been doing the remit to extend this and incorporate AI to teach tens of millions of kids around the world that will be open source, hosted at the UN. One laptop per child, but really one AI per child.</p> <p>That's one of my main focuses because I think I did a podcast about this. A lot of people talk about human rights and ethics and morals and things like that. One of the frames I found really interesting from Vinay Gupta, who's a bit of a crazy guy, but a great thinker, was that we should think about human rights in terms of the rights of children because they don't have any agency and they can't control things and what is their right to have a climate, what is their right to food and education and other things. We should really provide for them and I'm going to use this technology to provide for them so there's literally no child left behind, they have access to all the tools and technology they need.</p> <p>That's why creativity was a core component of that and communication, education and healthcare. Again, it's not just us, all we are is the catalyst and it's the community that comes and helps and extends that.</p> <p>If you'd like to learn more about our education initiative, they're at Magic Worldwide. Lots more on that soon as we scale up to tens of millions of kids.</p> <p>My question was about whether I have to pass down the rail license limitations when licensing SD based images or I can release as good. (45:06)</p> <p>Ah yes, you don't have to do rail license, you can release as is. It's only if you are running the model or distributing the model to other people that you have to do that.</p> <p>As a composer and audio engineer myself, I cannot imagine AI will approach the emotional intricacies and depths of complexity found in music by world class musicians, at least not anytime soon. That said, I'm interested in AI as a tool, would love to explore how it can be used to help in this production process. Is stability AI involved in this?</p> <p>Yes we are, I think someone just linked to harmonai when I play and we will be releasing a whole suite of tools soon to extend the capability of musicians and make more people into musicians.</p> <p>And this is one of the interesting ones, like these models, they pay attention to the important parts of any media.</p> <p>So there's always this question about expressivity and humanity, I mean they are trained on humanity and so they resonate and I think that's something that you kind of have to acknowledge and then it's about aesthetics have been solved to a degree by this type of AI. So something can be aesthetically pleasing, but aesthetics are not enough. If you are an artist, a musician or otherwise, I'd say a coder, it's largely about narrative and story.</p> <p>And what does that look like around all of this? Because things don't exist in a vacuum, it can be a beautiful thing or a piece of music, but you remember it because you were driving a car when you were 18 with your best friends, you know, or it was at your wedding or something like that.</p> <p>That's when story matters, for music, for art, for other things as well like that.</p> <p>Are you guys working on LMs as well, something to compete with OpenAI GPT-3?</p> <p>Yes.</p> <p>We recently released from the Carpa Lab, the instruct framework and we are training to achieve chiller optimal models, which outperformed GPT-3 on a fraction of the parameters. They will get better and better and better. And then as we create localized data sets and the education data sets, those are ideal for training foundation models at ridiculous power relative to the parameters. So I think that it will be pretty great to say the least as we kind of focus on that. EleutherAI, which was the first community that we properly supported and a number of stability employees help lead that community.</p> <p>The focus was GPT Neo and GPT-J, which were the open source implementations of GPT-3 but on a smaller parameter scale, which had been downloaded 25 million times by developers, which I think is a lot more use than GPT-3 has got. But GPT-3 is fantastic or instruct GPT, which it really is. I think this instruct model that took it down a hundred times. Again, if you're technical, you can look at the Carpa community and you can see the framework around that.</p> <p>In the future for other models, we are building an opt-in and opt-out system for artists and others that will lead to use in partnerships leading organizations. This model has some principles, the outputs are not direct for any single piece or initiatives of motion with regards to this.</p> <p>There will be announcements next week about this and various entities that we're bringing in place for that. That's all I can say, because I'm not allowed to spoil announcements, but we've been working super hard on this.</p> <p>I think there's two or maybe three announcements, it'll be 17th and 18th will be the dates of those.</p> <p>When will stability and EleutherAI be able to translate geese to speech in real time?</p> <p>I think the kind of honking models are very complicated. Actually, this is actually very interesting. People have actually been using diffusion models to translate animal speech and understand  it. If you look at something like whisper, it might actually be in reach. Whisper by open AI, they open sourced it kindly, I wonder what caused them to do that, is a fantastic speech to text model. One of the interesting things about it is you can change the language you're speaking in the middle of a sentence and it'll still pick that up. So if you train it enough, then you'll be able to kind of do that. So one of the entities we're talking with wants to train based on whale song to understand whales. Now this sounds a bit like Star Trek, but that's okay, I like Star Trek. So we'll see how that goes.</p> <p>Will dream studio front-end be open source so it can be used on local GPUs? (50:25)</p> <p>I do not believe there's any plans for that at the moment because dream studio is kind of our pro CMR end kind of thing, but you'll see more and more local GPU usage. So like, you know, you've got visions of chaos at the moment on windows machines by softology is fantastic, where you can run just about any of these notebooks like D forum and others or HLKY or whatever. And so I think that's kind of a good step. Similarly, if you look at the work being done on the Photoshop plugin, it will have local inference in a week or two. So you can use that directly from Photoshop and soon many other plugins.</p> <p>What do you think of the situation where a Google engineer believed the AI chatbot achieved sentience?</p> <p>It did not. He was stupid. Unless you have a very low bar of sentience pose, you could, I mean, some people are barely sentient. It must be said, especially when they're arguing on the internet, never went an argument on the internet. That's another thing like facts don't really work on the internet. A lot of people have preconceived notions. Instead, you should try to just be like, you know, as open minded as possible and let people agree to disagree.</p> <p>Thoughts on getting seamless equirectangular 360 degree and 180 degree and HDR outputs in one shot for image to text and text to image.</p> <p>I mean, you could use things like, I think I called it stream fusion, which was dream fusions, stable diffusion kind of combined. There are a bunch of data sets that we're working on to enable this kind of thing, especially from GoPro and others. But I think it'd probably be a year or two away still.</p> <p>Any plans for text-to-3d diffusion models?</p> <p>Yes, there are. And they are in the works.</p> <p>With some of the recent backlash from artists, is there anything you wish that SD did differently in the earliest stages that would have changed the framing around image synthesis?</p> <p>No, really. I mean, like the point is that these things can be fine-tuned anyway. So I think people have attacked fine tuning. I mean, ultimately it's like, I understand the fear, this is threatening to their jobs and the thing cause anyone can kind of do it, but it's not like ethically correct for them to say, actually, we don't want everyone to be artists. So instead they focus on, it's taken my art and trained on my art and you know, it's impossible for this to work without my art. Not really.</p> <p>So you train on ImageNet and it can still create just about any composition. Part of the problem was having the clip model embedded in there because the clip model knows a lot of stuff. We don't know what's in the open AI dataset, um, as should we do kind of, and it's interesting.  think that all we can do is kind of learn from the feedback from the people that aren't shouting at us or like, uh, you know, members of the team have received death threats and other things which are completely over the line. This is a reason why I think caution is the better part of what we're doing right now. You know, we have put ourselves in our way, like my inbox does look a bit ugly, uh, in certain places, to try and calm things down and really listen to the calmer voices there and try and build systems so people can be represented appropriately. It's not an easy question. I think it's incumbent on us to try and help facilitate this conversation because it's an important question.</p> <p>Are you looking to decentralize GPU AI compute?</p> <p>Uh, yeah, we've got kind of models that enable that, um, hive minds that you'll see, um, on the decentralized learning side as an example whereby I'm trained on distributed GPUs, um, actually models. I think that we need the best version of that is on reinforcement learning models. I think those are deep learning models, especially when considering things like, uh, community models, et cetera, because as those proliferate and create their own custom models bind to your dream booth or others, there's no way that centralized systems can keep up. But I think decentralized compute is pretty cheap though.</p> <p>Are we going to do nerf type models?</p> <p>Yes. I think nerfs are going to be the big thing. They are, um, going to be supported by Apple and Apple hardware. So I think you'll see lots of nerf type models there.</p> <p>Will AI lead to UBI? (56:09)</p> <p>Maybe. It'll either lead to UBI and utopia or panopticon that we can never escape from because the models that were previously used to focus our attention and service ads will be used to control our brains instead. And they're really good at that. So, you know, no big deal, just two forks in the road. That's the way we kind of do.</p> <p>When will we be able to generate games with AI?</p> <p>You can already generate games with AI. So the code models allow you to create basic games, but then we've had generative games for many years already.</p> <p>How's your faith influence your mission?</p> <p>I mean, it's just like all faiths are the same. Do you want to others as you'd have done unto yourself, right?</p> <p>The golden rule, um, for all the stuff around there. I think people forget that we are just trying to do our best. Like it can lead to bad things though.</p> <p>So Robert chief rabbi, Jonathan Sacks, sadly past very smart guy had this concept of altruistic evil with people who tried to do good, can do the worst evil because they believe they're doing good. No one wants to be in our soul and bad, even if we have our arguments and it makes us forget our humanity.</p> <p>What I really want to focus on is this idea of public interest and bring this technology to the masses because I don't want to have this world where I looked at the future and there's this AI God that is controlled by a private enterprise. Like that enterprise would be more powerful than any nation unelected and in control of everything.</p> <p>And that's not a future that I want from my children.</p> <p>I think, because I would not want that done unto me and I think it should be made available for people who have different viewpoints to me as well. This is why, like I said, look, I know that there was a lot of tension over the weekend and everything on the community, but we really shouldn't be the only community for this. And we don't want to be the sole arbiter of everything here.</p> <p>We're not open AI or deep mind or anyone like that. We're really trying to just be the catalyst to build ecosystems where you can find your own place, whether you agree with us or disagree with us. Um, so yeah, I think that also it'd be nice when people of other faiths or no faith can actually talk together reasonably. Um, and that's one of the reasons that we accelerated AR and faith.org. Again, you don't have to agree with it, but just realize these are some of the stories that people subscribe to and everyone's got their own faith in something or other, literally not.</p> <p>How are you going to train speed cost and TPUs versus a one hundreds or the cost of switching TensorFlow from PyTorch?</p> <p>We have code that works on both. And we have had great results on TPU V4s, the horizontal and vertical scaling works really nicely. And gosh, there is something called a V5 coming soon. That'd be interesting. Um, you will see models trained across a variety of different architectures and we're trying just about all the top ones there.</p> <p>Does StabilityAI have plans to take on investors at any point or have they already?</p> <p>We have taken on investors. There will be an announcement on that.</p> <p>We have given up zero control and we will not give up any control. I am very good at this.</p> <p>As I mentioned previously, the original stable diffusion model was financed by some of the leading AI artists in the world and collectors. And so, you know, we've been kind of community focused.</p> <p>I wish that we could do a token sale or an IPO or something and be community focused, but it just doesn't fit with regulations right now. So anything that I can say is that we will and will always be independent.</p> <p>How much of an impact do you think AI will impact neural implant cybernetics? It appears one of the limiting facts of cybernetics is the input method, not necessarily the hardware. (1:00:24)</p> <p>I don't know.</p> <p>I guess you have no idea too much, I never thought about that. Like I think that it's probably required for the interface layer. The way that you should look at this technology is that you've got the highest structure to the unstructured world, right? And this acts as a bridge between it.</p> <p>So like with stable diffusion, you can communicate in images that you couldn't do otherwise. Cybernetics is about the kind of interface layer between humans and computers. And again, you're removing that in one direction and the cybernetics allow you to remove it in the other direction. So you're going to have much better information flow. So I think it will have a massive impact from these foundation devices.</p> <p>Can you make cyberpunk 2077 not broken?</p> <p>I was the largest investor in CD project at one point and it is a crying shame what happened there. Uh, I have a lot of viewpoints on that one. But you know, we can create like cyberpunk worlds of our own in what did I say? Five years. Yeah. Not Elon Musk in there. So that's going to be pretty exciting.</p> <p>Are you guys planning on creating any hardware devices? A more consumer-oriented one, which has AI as OS?</p> <p>Uh, we have been looking into customized ones. Um, so some of the kind of edge architecture, but it won't be for a few years on the AI side. Actually, that will be, it'll probably be towards the next year because we've got thaton our tablets. So we've got basically a fully integrated stack or tablets for education, healthcare, and others. And again, we were trying to open source as much as possible. So looking to risk five and alternative architectures there. probably announcement there in Q1, </p> <p>Anything specific you'd like to see out of the community?</p> <p>I just like people to be nice to each other, right? Like communities are hard. It's hard to scale community.</p> <p>Like humans are designed for one to 150 and what happens is that as we scale communities bigger than that, this dark monster of our being, Moloch, kind of comes out. People get like really angsty and there's always going to be education, there's always going to be drama. How many communities do you know that aren't drama and like, just consider what your aunts do and they chat all the time. It's all kind of drama.</p> <p>I like to focus on being positive and constructive as much as possible and acknowledging that everyone is bored humans. But again, sometimes you make tough decisions. I made a tough decision this weekend. It might be right. It might be wrong, but you know, it's what I thought was best for the community. We wanted to have checks and balances and things, but it's a work in progress. Like I don't know how many people we've got in the community right now, like 60,000 or something like that. That's a lot of people and you know, I think it's, 78,000, that's a lot of fricking people. That's like a small town in the US or like a city in Finland or something like that. I just like people to be excellent to each other.</p> <p>How are you Emad?</p> <p>I'm a bit tired.</p> <p>Back in London for the first time in a long time, I was traveling, trying to get the education thing set up. There's a stability Africa set up as well. There's some work that we're doing in Lebanon, which unfortunately is really bad. I said stability does a lot more than image and it's just been a bit of a stretch even now with a hundred people.</p> <p>The reason that we're doing everything so aggressively is cause you kind of have to because there's just a lot of unfortunateness in the world. And I think you'd feel worse about yourself if you don't have to.</p> <p>And there's an interesting piece I read recently, it's like, I know Simon freed, uh, FTX, you know, he's got this thing about effective altruism. He talks about this thing of expected utility. How much impact can you make on the world? And you have to make big bets. So I made some really big bets. I put all my money into fricking GPU's. I really created together a team. I got government international backing and a lot of stuff because I think you, everyone has agency and you have to figure out where you can add the most agency and accelerate things up there. Uh, we have to bring in the best systems and we've built this multivariate system with multiple communities and now we're doing joint ventures in every single country because we think that is a whole new world.</p> <p>There's another great piece Sequoia did recently about generative AI being a whole new world that will create trillions. We're at this tipping point right now. And so I think unfortunately you've got to work hard to do that because it's a once in a lifetime opportunity.</p> <p>Just like everyone in this community here has a once in a lifetime opportunity. You know about this technology that how many people in your community know about now?</p> <p>Everyone in the world, everyone that you know will be using this in a few years and no one knows the way it's going to go.</p> <p>What's a good way to handle possible tribalism, extremism?</p> <p>So if you Google me and me, my name, you'll see me writing in the wall street journal and Reuters and all sorts of places about counter extremism. It's one of my expert topics and unfortunately it's difficult with the social media echo changers to kind of get out of that and you find people going in loops because sometimes things aren't fair.</p> <p>Like, you know, again, let's take our community. For example, this weekend actions were taken, you know, the banning that we could sit down fair. And again, that's understandable because it's not a cut and dry, easy decision. You had kind of the discussions going on loop. You had people saying some really unpleasant things, you know, some of the stuff made me kind of sad because I was exhausted and you know, people questioning my motivations and things like that.</p> <p>And again, it's your prerogative, but as a community member myself, it made me feel bad. I think the only way that you can really fight extremism and some things like that is to have checks and balances and processes in place. The mod team have been working super hard on that.</p> <p>I think this community has been really well-behaved, like, you know, it was super difficult and some of the community members got really burned out during the beta because they had to put up with a lot of shit, to put it quite simply. But getting people on the same page, getting a common mission and kind of having a degree of psychological safety where people can say what they want, which is really difficult in a community where you don't know where everyone is. That's the only way that you can get around some of this extremism and some of this hate element.</p> <p>Again, I think the common mission is the main thing. I think everyone here is in a common mission to build cool shit, create cool shit. And you know, like I said, the tagline kind of create, don't hate, right?</p> <p>Emad, real-life meetups for us members?</p> <p>Yeah, we're going to have little stability societies all over the place and hackathons. We're just putting an events team together to really make sure they're well organized and not our usual disorganized shambles. But you know, feel free to do it yourselves, you know, like, we're happy to amplify it when community members take that forward.</p> <p>And the things we're trying to encourage are going to be like artistic oriented things, get into the real world, go and see galleries, go and understand things, go and paint, that's good painting lessons, etc. As well as hackathons and all this more techy stuff, techy kind of stuff. You can be part of the events team by messaging careers at stability.ai. Again, we will have a careers page up soon with all the roles, we'll probably go to like 250 people in the next few months. And yeah, it's going very fast.</p> <p>Any collaboration in China yet? Can we use Chinese clip to guide the current one or do we need to retrain the model, embed the language clip into the model?</p> <p>I think you'll see a Chinese variant of stable diffusion coming out very soon. Can't remember what the current status is. We do have a lot of plans in China, we're talking to some of the coolest entities there. As you know, it's difficult due to sanctions and the Chinese market, but it's been heartening to see the community expand in China so quickly.</p> <p>And again, as it's open source, it didn't need us to go in there to kind of do that. I'd say that on the community side, we're going to try and accelerate a lot of the engagement things.</p> <p>I think that the Doctor Fusion one's ongoing, you know, shout out to Dreitweik for Nerf Gun and Almost 80 for kind of the really amazing kind of output there.</p> <p>I don't think we do enough to appreciate the things that you guys post up and simplify them. And I really hope we can do better in future. The mod team are doing as much as they can right now. And again, will we try to amplify the voices of the artistic members of our community as well, more and more, and give support through grants, credits, events and other things as we go forward.</p> <p>Is there going to be a time when we have AI friends we create ourselves, personal companions speaking to us via our monitor, much of the same way a webcam call is done, high quality, et cetera?</p> <p>Yes, you will have \"her\" from Joachim Phoenix's movie, Her, with Scarlett Johansson whispering in your ear. Hopefully she won't dump you at the end, but you can't guarantee that.</p> <p>If you look at some of the text to speech being emotionally resonant, then, you know, it's kind of creepy, but it's very immersive. So I think voice will definitely be there first. Again, try talking to a character.AI model and you'll see how good some of these chat bots can be. There are much better ones coming. We've seen this already with Xiaoshi in China, so Alice, which a lot of people use for mental health support and then Elisa in Iran. So millions of people use these right now as their friends. Again, it's good to have friends.</p> <p>Again, we recommend 7cups.com if you want to have someone to talk to, but it's not the same person each time or, you know, like just going out and making friends, but it's not easy.</p> <p>I think this will help a lot of people with their mental health, etcetera.</p> <p>How early do you think we are in this AI wave that's emerging? With how fast it's changing it's hard not to feel FOMO. (1:10:28)</p> <p>It is actually literally exponential.</p> <p>So like when you do a log normal return of the number of AI papers that are coming out, it's a straight line. So it's literally an exponential kind of curve. Like I can't keep up with it. No one can keep up with it. We have no idea what's going on. And the technology advances like there's that meme. Like one hour here is seven years on earth. Like from interstellar, that's how life kind of feels like I was on top of it for a few years and now it's like, I didn't even know what's happening. Here we go.</p> <p>It's a doubling rate of 24 months. It's a bit insane.</p> <p>Any comments on Harmony AI? How close do you think we are to having music sound AI with the same accessibility afforded by stable diffusion?</p> <p>Now, Harmony has done a slightly different model of releasing dance diffusion gradually. We're putting it out there as we license more and more data sets, some of the O and X and other work that's going on.</p> <p>I mean, basically considering that you're at the VQGAN moment right now, if you guys can remember that from all of a year ago or 18 months ago, it'll go exponential again because the amount of stuff here is going to go crazy.</p> <p>Like generative AI, look at that Sequoia link I posted is going to be the biggest investment theme of the next few years and literally tens of billions of dollars are going to be deployed like probably next year alone into this sector.</p> <p>And most of it will go to stupid stuff, some will go to good stuff, most will go to stupid stuff but a decent amount will go to forwarding music in particular because the interesting thing about musicians is that they're already digitally intermediated versus artists who are not.</p> <p>So artists, some of them use Procreate and Photoshop, a lot of them don't. But musicians use synthesizers and DSPs and software all the time. So it's a lot easier to introduce some of these things to their workflow and then make it accessible to the people. Yeah, musicians just want more snares. You see the drum bass guy there.</p> <p>When will you launch the full DreamStudio and will it be able to do animations? If so, do you think it'll be more cost effective than using Colab?</p> <p>Very soon, yes, and yes, there we go. Keep an eye here.</p> <p>Then the next announcements won't be hopefully quite so controversial, but instead very exciting,</p> <p>Do you think an AI therapist could be something to address the lack of access to qualified mental health experts?</p> <p>I would rather have volunteers augmented by that. So again, with 7Cups.com, we have 480,000 volunteers helping 78 million people each month train on active listening that hopefully will augment by AI as we help them build their models.</p> <p>AI can only go so far, but the edge cases and the failure cases I think are too strong. And I think again, a lot of care needs to be taken around that because people's mental health is super important.</p> <p>At the same time, we're trialing art therapy with stable diffusion as a mental health adjunct in various settings from survivors of domestic violence to veterans and others. And I think it will have amazing results because there's nothing quite like the magic of using this technology.</p> <p>And I think, again, magic is kind of the operative word here that we have. That's how you know technology is cool.</p> <p>What are your thoughts on Buckminster Fuller's work and his thoughts on how to build a world that doesn't destroy himself?</p> <p>To be honest, I'm not familiar with it.</p> <p>But I think the world is destroying itself at the moment and we've got to do everything we can to stop it. Again, I mentioned earlier, one of the nice frames I've thought about this is really thinking about the rights of children because they can't defend themselves. And are we doing our big actions with a view to the rights of those children? I think that children have a right to this technology and that's every child, not just ones in the West.</p> <p>And that's why I think we need to create personalized systems for them and infrastructure so they can go up and kind of get out.</p> <p>How will generative models and unlimited custom tailored content to an audience of one impact how we value content? The paradox of choice is more options tend to make people more anxious and we get infinite choice right now. How do we get adapted to our new god-like powers in this hedonic treadmill? Is it a net positive for humanity? How much consideration are we given to potential bad outcomes?</p> <p>I think this is kind of one of those interesting things whereby, like I was talking to Alexander Wang at scale about this and he posted something on everyone being in their own echo chambers as you basically get hedonic to death, entertained to death. Kind of like this WALL-E, you remember the fat guys with their VR headsets? Yeah, kind of like that. I don't think that's the case.</p> <p>I think people will use this to create stories because we're prosocial narrative creatures and the n equals one echo chambers are a result of the existing internet without intelligence on the edge. We want to communicate unless you have Asperger's like me and social communication disorder, in which case communicating is actually quite hard, but we learned how to do it. And I think, again, we're prosocial creatures that love seeing people listen to what we do.</p> <p>You've got likes and, you know, you've got this kind of hook model where you input something you're triggered and then you wait for verification and validation. So I think actually this will allow us to create our stories better and create a more egalitarian internet because right now the internet itself is this intelligence amplifier that means that some of the voices are more heard than others because some people know how to use the internet and they drown out those who do not and a lot of people don't even have access to this, so yeah.</p> <p>When do you think multi-models will emerge combining language, video and image?</p> <p>I think they'll be here by Q1 of next year and they'll be good.</p> <p>I think that by 2024 they'll be truly excellent.</p> <p>You can look at the DeepMind Gato paper on the autoregression of different modalities on reinforcement learning to see some of the potential on this. So Gato is just a 1.3 billion parameter model that is a generalist agent.</p> <p>As we've kind of showed by merging image and others, these things can cross-learn just like humans and I think that's fascinating and that's why we have to create models for every culture, for every country, for every individual so we can learn from the diversity and plurality of humanity to create models that are aligned for us instead of against us.</p> <p>And I think that's much better than stack more layers and build giant freaking supercomputers to train models to serve ads or whatever.</p> <p>wrap up</p> <p>So with that, I bid you adieu. It's been seven weeks, feels like seven years or seven minutes, I'm not even sure anymore, like I think we made a time machine. But hopefully we can start building stuff a lot more structured. So thanks all and you know, stay cool, rock on, bye.</p>"},{"location":"media/","title":"Media","text":"<p>As seen in:  - 2023-02-05 - https://p.migdal.pl/blog/2023/02/ai-arts-information-theory/  - 2023-01-26 - YouTube - Live 96: InstructPix2Pix and STABLE DIFFUSION  - 2023-01-23 - Gigazine  - 2023-01-23 - Changelog.com - \"ImaginAIry imagines &amp; edits images from text inputs\"  - 2022-12-10 - YouTube - Live 91: TRANSCRIBE Audio with WHISPER and GENERATE images with STABLE DIFFUSION, Locally  - 2023-01-22 - Hacker News - Show HN: New AI edits images based on text instructions  - 2023-01-10 - YouTube - Build A Free AI Image Generator Bot in 20 minutes!  - 2022-11-24 - Hacker News - Stable Diffusion 2.0 on Mac and Linux via imaginAIry Python library</p> <p>Used by:  - imageeditor.ai</p>"},{"location":"todo/","title":"Todo","text":""},{"location":"todo/#todo","title":"Todo","text":""},{"location":"todo/#v14-todo","title":"v14 todo","text":"<ul> <li>\u2705 configurable composition cutoff</li> <li>\u2705 rename model parameter weights </li> <li>\u2705 rename model_config parameter to architecture and make it case insensitive</li> <li>\u2705 add --size parameter that accepts strings (e.g. 256x256, 4k, uhd, 8k, etc)</li> <li>\u2705 detect if cuda torch missing and give better error message</li> <li>\u2705 add method to install correct torch version</li> <li>\u2705 make cli run faster again</li> <li>\u2705 add tests for cli commands</li> <li>\u2705 add type checker</li> <li>\u2705 add interface for loading diffusers weights</li> <li>\u2705 SDXL support</li> <li>\u2705 sdxl inpainting </li> <li>t2i adapters</li> <li>image prompts</li> <li>embedding inputs</li> <li>save complete metadata to image</li> <li>recreate image from metadata</li> <li>auto-incoporate https://huggingface.co/madebyollin/sdxl-vae-fp16-fix</li> <li>only output the main image unless some flag is set</li> <li>\u2705 allow selection of output video format</li> <li>test on python 3.11</li> <li>allow specification of filename format</li> <li>chain multiple operations together imggen =&gt; videogen</li> <li> <p>https://github.com/pallets/click/tree/main/examples/imagepipe </p> </li> <li></li> <li> <p>https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic</p> </li> <li>make sure terminal output on windows doesn't suck</li> <li>add method to show cache size</li> <li>add method to clear model cache</li> <li>add method to clear cached items not recently used (does diffusers have one?)</li> <li>create actual documentation</li> </ul>"},{"location":"todo/#investigate","title":"Investigate","text":"<ul> <li>use fancy noise https://github.com/Extraltodeus/noise_latent_perlinpinpin</li> <li>use latent upscaler https://github.com/city96/SD-Latent-Upscaler</li> <li>use latent interposer https://github.com/city96/SD-Latent-Interposer/tree/main</li> <li>https://github.com/madebyollin/taesd</li> <li>textdiffusers https://jingyechen.github.io/textdiffuser2/</li> <li>Fast diffusion with LCM Lora https://huggingface.co/latent-consistency/lcm-lora-sdv1-5/tree/main</li> <li>3d diffusion https://huggingface.co/stabilityai/stable-zero123</li> <li>magic animate</li> <li>consistency decoder</li> <li>https://github.com/XPixelGroup/HAT</li> </ul>"},{"location":"todo/#old-todo","title":"Old Todo","text":"<ul> <li>Inference Performance Optimizations</li> <li>\u2705 fp16</li> <li>\u2705 Doggettx Sliced attention</li> <li>\u2705 xformers support https://www.photoroom.com/tech/stable-diffusion-100-percent-faster-with-memory-efficient-attention/</li> <li>https://github.com/neonsecret/stable-diffusion  </li> <li>https://github.com/CompVis/stable-diffusion/pull/177</li> <li>https://github.com/huggingface/diffusers/pull/532/files</li> <li>https://github.com/HazyResearch/flash-attention</li> <li>https://github.com/chavinlo/sda-node</li> <li> <p>https://github.com/AminRezaei0x443/memory-efficient-attention/issues/7</p> </li> <li> <p>Development Environment</p> </li> <li>\u2705 add tests</li> <li>\u2705 set up ci (test/lint/format)</li> <li>\u2705 unified pipeline (txt2img &amp; img2img combined)</li> <li>\u2705 setup parallel testing</li> <li>\u2705 add docs</li> <li>\ud83d\udeab remove yaml config</li> <li>\ud83d\udeab delete more unused code</li> <li>faster latent logging https://discuss.huggingface.co/t/decoding-latents-to-rgb-without-upscaling/23204/9</li> <li>Interface improvements</li> <li>\u2705 init-image at command line</li> <li>\u2705 prompt expansion</li> <li>\u2705 interactive cli</li> <li>Image Generation Features</li> <li>\u2705 add k-diffusion sampling methods</li> <li>\u2705 tiling</li> <li>\u2705 generation videos/gifs</li> <li>\u2705 controlnet<ul> <li>scribbles input</li> <li>segmentation input</li> <li>mlsd input</li> </ul> </li> <li>Attend and Excite</li> <li>Compositional Visual Generation<ul> <li>https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch</li> <li>https://colab.research.google.com/github/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch/blob/main/notebooks/demo.ipynb#scrollTo=wt_j3uXZGFAS</li> </ul> </li> <li>\u2705 negative prompting<ul> <li>some syntax to allow it in a text string</li> </ul> </li> <li>paint with words<ul> <li>https://github.com/cloneofsimo/paint-with-words-sd </li> </ul> </li> <li>https://multidiffusion.github.io/</li> <li>images as actual prompts instead of just init images. <ul> <li>not directly possible due to model architecture.</li> <li>can it just be integrated into sampler? </li> <li>requires model fine-tuning since SD1.4 expects 77x768 text encoding input</li> <li>https://twitter.com/Buntworthy/status/1566744186153484288</li> <li>https://github.com/justinpinkney/stable-diffusion</li> <li>https://github.com/LambdaLabsML/lambda-diffusers</li> <li>https://www.reddit.com/r/MachineLearning/comments/x6k5bm/n_stable_diffusion_image_variations_released/</li> </ul> </li> <li>Image Editing</li> <li>\u2705outpainting<ul> <li>https://github.com/parlance-zz/g-diffuser-bot/search?q=noise&amp;type=issues</li> <li>lama cleaner</li> </ul> </li> <li>\u2705 inpainting<ul> <li>https://github.com/Jack000/glid-3-xl-stable </li> <li>https://github.com/andreas128/RePaint</li> <li>\u2705 img2img but keeps img stable</li> <li>https://www.reddit.com/r/StableDiffusion/comments/xboy90/a_better_way_of_doing_img2img_by_finding_the/</li> <li>https://gist.github.com/trygvebw/c71334dd127d537a15e9d59790f7f5e1</li> <li>https://github.com/pesser/stable-diffusion/commit/bbb52981460707963e2a62160890d7ecbce00e79</li> <li>https://github.com/SHI-Labs/FcF-Inpainting https://praeclarumjj3.github.io/fcf-inpainting/</li> </ul> </li> <li>\u2705 text based image masking<ul> <li>\u2705 ClipSeg - https://github.com/timojl/clipseg</li> <li>https://github.com/facebookresearch/detectron2</li> <li>https://x-decoder-vl.github.io/</li> </ul> </li> <li>Maskless editing<ul> <li>\u2705 instruct-pix2pix</li> <li></li> </ul> </li> <li>Attention Control Methods<ul> <li>https://github.com/bloc97/CrossAttentionControl</li> <li>https://github.com/ChenWu98/cycle-diffusion</li> </ul> </li> <li>Image Enhancement</li> <li>Photo Restoration - https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life</li> <li>Upscaling<ul> <li>\u2705 realesrgan </li> <li>ldm</li> <li>https://github.com/lowfuel/progrock-stable</li> <li>txt2imghd</li> <li>latent scaling + reprocessing</li> <li>stability upscaler</li> <li>rivers have wings upscaler</li> <li>stable super-res?</li> <li>todo: try with 1-0-0-0 mask at full image resolution (rencoding entire image+predicted image at every step)</li> <li>todo: use a gaussian pyramid and only include the \"high-detail\" level of the pyramid into the next step</li> <li>https://www.reddit.com/r/StableDiffusion/comments/xkjjf9/upscale_to_huge_sizes_and_add_detail_with_sd/</li> </ul> </li> <li>\u2705 face enhancers<ul> <li>\u2705 gfpgan - https://github.com/TencentARC/GFPGAN</li> <li>\u2705 codeformer - https://github.com/sczhou/CodeFormer</li> </ul> </li> <li>\u2705 image describe feature - <ul> <li>\u2705 https://github.com/salesforce/BLIP</li> <li>\ud83d\udeab CLIP brute-force prompt reconstruction</li> <li>The accuracy of this approach is too low for me to include it in imaginAIry</li> <li>https://github.com/rmokady/CLIP_prefix_caption</li> <li>https://github.com/pharmapsychotic/clip-interrogator (blip + clip)</li> <li>https://github.com/KaiyangZhou/CoOp</li> </ul> </li> <li>\ud83d\udeab CPU support.  While the code does actually work on some CPUs, the generation takes so long that I don't think it's     worth the effort to support this feature</li> <li>\u2705 img2img for plms</li> <li>\u2705 img2img for kdiff functions</li> <li>Other</li> <li>Enhancement pipelines</li> <li>text-to-3d https://dreamfusionpaper.github.io/<ul> <li>https://shihmengli.github.io/3D-Photo-Inpainting/</li> <li>https://github.com/thygate/stable-diffusion-webui-depthmap-script/discussions/50</li> <li>Depth estimation</li> <li>what is SOTA for monocular depth estimation?</li> <li>https://github.com/compphoto/BoostingMonocularDepth</li> </ul> </li> <li>make a video https://github.com/lucidrains/make-a-video-pytorch</li> <li>animations<ul> <li>https://github.com/francislabountyjr/stable-diffusion/blob/main/inferencing_notebook.ipynb</li> <li>https://www.youtube.com/watch?v=E7aAFEhdngI</li> <li>https://github.com/pytti-tools/frame-interpolation</li> </ul> </li> <li>guided generation <ul> <li>https://colab.research.google.com/drive/1dlgggNa5Mz8sEAGU0wFCHhGLFooW_pf1#scrollTo=UDeXQKbPTdZI</li> <li>https://colab.research.google.com/github/aicrumb/doohickey/blob/main/Doohickey_Diffusion.ipynb#scrollTo=PytCwKXCmPid</li> <li>https://github.com/mlfoundations/open_clip</li> <li>https://github.com/openai/guided-diffusion</li> </ul> </li> <li>image variations https://github.com/lstein/stable-diffusion/blob/main/VARIATIONS.md</li> <li>textual inversion <ul> <li>https://www.reddit.com/r/StableDiffusion/comments/xbwb5y/how_to_run_textual_inversion_locally_train_your/</li> <li>https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb#scrollTo=50JuJUM8EG1h</li> <li>https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_textual_inversion_library_navigator.ipynb</li> <li>https://github.com/Jack000/glid-3-xl-stable</li> </ul> </li> <li>fix saturation at high CFG https://www.reddit.com/r/StableDiffusion/comments/xalo78/fixing_excessive_contrastsaturation_resulting/</li> <li>https://www.reddit.com/r/StableDiffusion/comments/xbrrgt/a_rundown_of_twenty_new_methodsoptions_added_to/</li> <li>\u2705 deploy to pypi</li> <li>find similar images https://knn5.laion.ai/?back=https%3A%2F%2Fknn5.laion.ai%2F&amp;index=laion5B&amp;useMclip=false</li> <li>https://github.com/vicgalle/stable-diffusion-aesthetic-gradients</li> <li>Training</li> <li>Finetuning \"dreambooth\" style</li> <li>Textual Inversion<ul> <li>Fast Textual Inversion </li> </ul> </li> <li>Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning (LORA)<ul> <li>https://huggingface.co/spaces/lora-library/Low-rank-Adaptation </li> </ul> </li> <li>Performance Improvements<ul> <li>ColoassalAI - almost got it working but it's not easy enough to install to merit inclusion in imaginairy. We should check back in on this.</li> <li>Xformers</li> <li>Deepspeed</li> </ul> </li> </ul>"},{"location":"todo/#decided-against","title":"Decided against","text":"<ul> <li>Scalecrafter https://yingqinghe.github.io/scalecrafter/  - doesn't look any better than img2img</li> </ul>"},{"location":"docs/CLI/colorize/","title":"aimg colorize","text":"<p>Colorize images using AI. Doesn't work very well yet.</p> <p>Usage:</p> <pre><code>aimg colorize [OPTIONS] [IMAGE_FILEPATHS]...\n</code></pre> <p>Options:</p> <pre><code>  --caption TEXT         Description of the photo. If not provided, it will be\n                         generated automatically.\n  -r, --repeats INTEGER  How many times to repeat the renders. If you provide\n                         two prompts and --repeat=3 then six images will be\n                         generated.  [default: 1]\n  --outdir PATH          Where to write results to.  [default:\n                         ./outputs/colorized]\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"docs/CLI/describe/","title":"aimg describe","text":"<p>Generate text descriptions of images.</p> <p>Usage:</p> <pre><code>aimg describe [OPTIONS] [IMAGE_FILEPATHS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"docs/CLI/edit/","title":"aimg edit","text":"<p>Edit an image via AI.</p> <p>Provide paths or URLs to images and directions on how to alter them.</p> <p>Example: aimg edit --prompt \"make the dog red\" my-dog.jpg my-dog2.jpg</p> <p>Same as calling <code>aimg imagine --model edit --init-image my-dog.jpg --init-image-strength 1</code> except this command can batch edit images.</p> <p>Usage:</p> <pre><code>aimg edit [OPTIONS] PATH|URL\n</code></pre> <p>Options:</p> <pre><code>  --image-strength FLOAT          Starting image strength. Between 0 and 1.\n  -p, --prompt TEXT               [required]\n  --model-weights-path, --model TEXT\n                                  Model to use. Should be one of flux,\n                                  miniaturuspotentia, miniaturuspotentia12,\n                                  modern-disney, modern-disney-15, modi,\n                                  modi15, mp, mp12, od, odv11, oj, oj1, oj2,\n                                  oj4, ojv1, ojv2, ojv4, opendalle,\n                                  opendalle11, openjourney, openjourney-v1,\n                                  openjourney-v2, openjourney-v4,\n                                  openjourney1, openjourney2, openjourney4,\n                                  potentia, potentia12, sd-1.5,\n                                  sd-1.5-inpaint, sd-15, sd-15-inpaint, sd-xl,\n                                  sd-xlinpaint, sd1.5, sd1.5-inpaint,\n                                  sd1.5inpaint, sd15, sd15-inpaint,\n                                  sd15inpaint, sdxl, sdxl-inpaint,\n                                  sdxlinpaint, or a path to custom weights.\n                                  [default: sd15]\n  --negative-prompt TEXT          Negative prompt. Things to try and exclude\n                                  from images. Same negative prompt will be\n                                  used for all images. A default negative\n                                  prompt is used if none is selected.\n  --prompt-strength FLOAT         How closely to follow the prompt. Image\n                                  looks unnatural at higher values  [default:\n                                  7.5]\n  --image-prompt PATH|URL         Starting image.\n  --image-prompt-strength FLOAT   Starting image strength. Between 0 and 1.\n  --outdir PATH                   Where to write results to.  [default:\n                                  ./outputs]\n  --output-file-extension [jpg|png]\n                                  Where to write results to.  [default: jpg]\n  -r, --repeats INTEGER           How many times to repeat the renders. If you\n                                  provide two prompts and --repeat=3 then six\n                                  images will be generated.  [default: 1]\n  --size TEXT                     Image size as a string. Can be a named size,\n                                  WIDTHxHEIGHT, or single integer. Should be\n                                  multiple of 8. Examples: 512x512, 4k, UHD,\n                                  8k, 512, 1080p\n  --steps INTEGER                 How many diffusion steps to run. More steps,\n                                  more detail, but with diminishing returns.\n  --seed INTEGER                  What seed to use for randomness. Allows\n                                  reproducible image renders.\n  --upscale\n  --fix-faces\n  --fix-faces-fidelity FLOAT      How faithful to the original should face\n                                  enhancement be. 1 = best fidelity, 0 = best\n                                  looking face.\n  --solver, --sampler [ddim|dpmpp]\n                                  Solver algorithm to generate the image with.\n                                  (AKA 'Sampler' or 'Scheduler' in other\n                                  libraries.  [default: ddim]\n  --log-level [DEBUG|INFO|WARNING|ERROR]\n                                  What level of logs to show.  [default: INFO]\n  -q, --quiet                     Suppress logs. Alias of `--log-level ERROR`.\n  --show-work                     Output a debug images to `steps` folder.\n  --tile                          Any images rendered will be tileable in both\n                                  X and Y directions.\n  --tile-x                        Any images rendered will be tileable in the\n                                  X direction.\n  --tile-y                        Any images rendered will be tileable in the\n                                  Y direction.\n  --mask-image PATH|URL           A mask to use for inpainting. White gets\n                                  painted, Black is left alone.\n  --mask-prompt TEXT              Describe what you want masked and the AI\n                                  will mask it for you. You can describe\n                                  complex masks with AND, OR, NOT keywords and\n                                  parentheses. The strength of each mask can\n                                  be modified with {*1.5} notation.\n\n                                  Examples:   car AND (wheels{*1.1} OR trunk\n                                  OR engine OR windows OR headlights) AND NOT\n                                  (truck OR headlights){*10} fruit|fruit stem\n  --mask-mode [keep|replace]      Should we replace the masked area or keep\n                                  it?  [default: replace]\n  --mask-modify-original          After the inpainting is done, apply the\n                                  changes to a copy of the original image.\n  --outpaint TEXT                 Specify in what directions to expand the\n                                  image. Values will be snapped such that\n                                  output image size is multiples of 8.\n                                  Examples `--outpaint\n                                  up10,down300,left50,right50` `--outpaint\n                                  u10,d300,l50,r50` `--outpaint all200`\n                                  `--outpaint a200`\n  --caption                       Generate a text description of the generated\n                                  image.\n  --precision [full|autocast]     Evaluate at this precision.  [default:\n                                  autocast]\n  --model-architecture TEXT       Model architecture. When specifying custom\n                                  weights the model architecture must be\n                                  specified. (sd15, sdxl, etc).\n  --prompt-library-path PATH      Path to folder containing phrase lists in\n                                  txt files. Use txt filename in prompt:\n                                  {_filename_}.\n  --version                       Print the version and exit.\n  --gif                           Create a gif of the generation.\n  --compare-gif                   Create a gif comparing the original image to\n                                  the modified one.\n  --arg-schedule TEXT             Schedule how an argument should change over\n                                  several generations. Format: `--arg-schedule\n                                  arg_name[start:end:increment]` or `--arg-\n                                  schedule arg_name[val,val2,val3]`\n  --compilation-anim [gif|mp4]    Generate an animation composed of all the\n                                  images generated in this run.  Defaults to\n                                  gif but `--compilation-anim mp4` will\n                                  generate an mp4 instead.\n  --caption-text TEXT             Specify the text to write onto the image.\n  --composition-strength FLOAT    Strength of the composition phase.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"docs/CLI/imagine/","title":"imagine","text":"<p>Generate images via AI.</p> <p>Can be invoked via either <code>aimg imagine</code> or just <code>imagine</code>.</p> <p>Usage:</p> <pre><code>imagine [OPTIONS] [PROMPT_TEXTS]...\n</code></pre> <p>Options:</p> <pre><code>  --negative-prompt TEXT          Negative prompt. Things to try and exclude\n                                  from images. Same negative prompt will be\n                                  used for all images.\n  --prompt-strength FLOAT         How closely to follow the prompt. Image\n                                  looks unnatural at higher values  [default:\n                                  7.5]\n  --init-image PATH|URL           Starting image.\n  --init-image-strength FLOAT     Starting image strength. Between 0 and 1.\n  --image-prompt PATH|URL         Starting image.\n  --image-prompt-strength FLOAT   Starting image strength. Between 0 and 1.\n  --outdir PATH                   Where to write results to.  [default:\n                                  ./outputs]\n  --output-file-extension [jpg|png]\n                                  Where to write results to.  [default: jpg]\n  -r, --repeats INTEGER           How many times to repeat the renders. If you\n                                  provide two prompts and --repeat=3 then six\n                                  images will be generated.  [default: 1]\n  --size TEXT                     Image size as a string. Can be a named size,\n                                  WIDTHxHEIGHT, or single integer. Should be\n                                  multiple of 8. Examples: 512x512, 4k, UHD,\n                                  8k, 512, 1080p\n  --steps INTEGER                 How many diffusion steps to run. More steps,\n                                  more detail, but with diminishing returns.\n  --seed INTEGER                  What seed to use for randomness. Allows\n                                  reproducible image renders.\n  --upscale\n  --fix-faces\n  --fix-faces-fidelity FLOAT      How faithful to the original should face\n                                  enhancement be. 1 = best fidelity, 0 = best\n                                  looking face.\n  --solver, --sampler [ddim|dpmpp]\n                                  Solver algorithm to generate the image with.\n                                  (AKA 'Sampler' or 'Scheduler' in other\n                                  libraries.  [default: ddim]\n  --log-level [DEBUG|INFO|WARNING|ERROR]\n                                  What level of logs to show.  [default: INFO]\n  -q, --quiet                     Suppress logs. Alias of `--log-level ERROR`.\n  --show-work                     Output a debug images to `steps` folder.\n  --tile                          Any images rendered will be tileable in both\n                                  X and Y directions.\n  --tile-x                        Any images rendered will be tileable in the\n                                  X direction.\n  --tile-y                        Any images rendered will be tileable in the\n                                  Y direction.\n  --allow-compose-phase / --no-compose-phase\n                                  Allow the image to be composed at a lower\n                                  resolution.\n  --mask-image PATH|URL           A mask to use for inpainting. White gets\n                                  painted, Black is left alone.\n  --mask-prompt TEXT              Describe what you want masked and the AI\n                                  will mask it for you. You can describe\n                                  complex masks with AND, OR, NOT keywords and\n                                  parentheses. The strength of each mask can\n                                  be modified with {*1.5} notation.\n\n                                  Examples:   car AND (wheels{*1.1} OR trunk\n                                  OR engine OR windows OR headlights) AND NOT\n                                  (truck OR headlights){*10} fruit|fruit stem\n  --mask-mode [keep|replace]      Should we replace the masked area or keep\n                                  it?  [default: replace]\n  --mask-modify-original          After the inpainting is done, apply the\n                                  changes to a copy of the original image.\n  --outpaint TEXT                 Specify in what directions to expand the\n                                  image. Values will be snapped such that\n                                  output image size is multiples of 8.\n                                  Examples `--outpaint\n                                  up10,down300,left50,right50` `--outpaint\n                                  u10,d300,l50,r50` `--outpaint all200`\n                                  `--outpaint a200`\n  --caption                       Generate a text description of the generated\n                                  image.\n  --precision [full|autocast]     Evaluate at this precision.  [default:\n                                  autocast]\n  --model-weights-path, --model TEXT\n                                  Model to use. Should be one of flux,\n                                  miniaturuspotentia, miniaturuspotentia12,\n                                  modern-disney, modern-disney-15, modi,\n                                  modi15, mp, mp12, od, odv11, oj, oj1, oj2,\n                                  oj4, ojv1, ojv2, ojv4, opendalle,\n                                  opendalle11, openjourney, openjourney-v1,\n                                  openjourney-v2, openjourney-v4,\n                                  openjourney1, openjourney2, openjourney4,\n                                  potentia, potentia12, sd-1.5,\n                                  sd-1.5-inpaint, sd-15, sd-15-inpaint, sd-xl,\n                                  sd-xlinpaint, sd1.5, sd1.5-inpaint,\n                                  sd1.5inpaint, sd15, sd15-inpaint,\n                                  sd15inpaint, sdxl, sdxl-inpaint,\n                                  sdxlinpaint, or a path to custom weights.\n                                  [default: sd15]\n  --model-architecture TEXT       Model architecture. When specifying custom\n                                  weights the model architecture must be\n                                  specified. (sd15, sdxl, etc).\n  --prompt-library-path PATH      Path to folder containing phrase lists in\n                                  txt files. Use txt filename in prompt:\n                                  {_filename_}.\n  --version                       Print the version and exit.\n  --gif                           Create a gif of the generation.\n  --compare-gif                   Create a gif comparing the original image to\n                                  the modified one.\n  --arg-schedule TEXT             Schedule how an argument should change over\n                                  several generations. Format: `--arg-schedule\n                                  arg_name[start:end:increment]` or `--arg-\n                                  schedule arg_name[val,val2,val3]`\n  --compilation-anim [gif|mp4]    Generate an animation composed of all the\n                                  images generated in this run.  Defaults to\n                                  gif but `--compilation-anim mp4` will\n                                  generate an mp4 instead.\n  --caption-text TEXT             Specify the text to write onto the image.\n  --composition-strength FLOAT    Strength of the composition phase.\n  --control-image PATH|URL        Image used for control signal in image\n                                  generation. For example if control-mode is\n                                  depth, then the generated image will match\n                                  the depth map extracted from the control\n                                  image. Defaults to the `--init-image`\n  --control-image-raw PATH|URL    Preprocessed image used for control signal\n                                  in image generation. Like `--control-image`\n                                  but  expects the already extracted signal.\n                                  For example the raw control image would be a\n                                  depth map orpose information.\n  --control-strength TEXT         Strength of the control signal.\n  --control-mode [|canny|depth|details|normal|hed|openpose|shuffle|edit|inpaint|colorize|qrcode|densepose]\n                                  how the control image is used as signal\n  --videogen                      Turns the generated photo into video\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"docs/CLI/upscale/","title":"aimg upscale","text":"<p>Upscale an image 4x using AI.</p> <p>Usage:</p> <pre><code>aimg upscale [OPTIONS] [IMAGE_FILEPATHS]...\n</code></pre> <p>Options:</p> <pre><code>  --format TEXT               Formats the file name. Default value will save\n                              '{original_filename}.upscaled{file_extension}'\n                              to the original directory.  {original_filename}:\n                              original name without the\n                              extension;{file_sequence_number:pad}: sequence\n                              number in directory, can make zero-padded (e.g.,\n                              06 for six digits).; {algorithm}: upscaling\n                              algorithm; {now:%Y-%m-%d:%H-%M-%S}: current date\n                              and time, customizable using standard strftime\n                              format codes. Use 'DEV' to config to save in\n                              standard imaginAIry format '{file_sequence_numbe\n                              r:06}_{algorithm}_{original_filename}.upscaled{f\n                              ile_extension}'.\n  --list-models               View available upscale models.\n  --upscale-model TEXT        Specify one or more upscale models to use.\n                              [default: realesrgan-x2-plus]\n  --fix-faces-fidelity FLOAT  How faithful to the original should face\n                              enhancement be. 1 = best fidelity, 0 = best\n                              looking face.\n  --fix-faces\n  --outdir PATH               Where to write results to. Default will be where\n                              the directory of the original file.  [default:\n                              ./outputs/upscaled]\n  --help                      Show this message and exit.\n</code></pre>"},{"location":"docs/CLI/videogen/","title":"aimg videogen","text":"<p>AI generate a video from an image</p> <p>Example:</p> <pre><code>aimg videogen --start-image assets/rocket-wide.png\n</code></pre> <p>Usage:</p> <pre><code>aimg videogen [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --start-image TEXT              Input path for image file.\n  --num-frames INTEGER            Number of frames.\n  -s, --steps INTEGER             Number of diffusion steps.\n  --model TEXT                    Model to use. One of: svd, svd-xt, svd-\n                                  image-decoder, svd-xt-image-decoder\n  --fps INTEGER                   FPS for the AI to target when generating\n                                  video\n  --size TEXT                     Video dimensions. Can be a named size,\n                                  single integer, or WIDTHxHEIGHT pair. Should\n                                  be multiple of 8. Examples: SVD, 512x512,\n                                  4k, UHD, 8k, 512, 1080p  [default: 1024,576]\n  --output-fps INTEGER            FPS for the output video\n  --output-format [webp|mp4|gif]  Output video format\n  --motion-amount INTEGER         How much motion to generate. value between 0\n                                  and 255.\n  -r, --repeats INTEGER           How many times to repeat the renders.\n                                  [default: 1]\n  --cond-aug FLOAT                Conditional augmentation.\n  --seed INTEGER                  Seed for random number generator.\n  --decoding_t INTEGER            Number of frames decoded at a time.\n  --output_folder TEXT            Output folder.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"docs/Python/ControlInput/","title":"ControlInput","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Pydantic model representing the input control parameters for an operation, typically involving image processing.</p> <p>This model includes parameters such as the operation mode, the image to be processed, an alternative raw image, and a strength parameter. It validates these parameters to ensure they meet specific criteria, such as the mode being one of the predefined valid modes and ensuring that both 'image' and 'image_raw' are not provided simultaneously.</p> <p>Attributes:</p> Name Type Description <code>mode</code> <code>str</code> <p>The operation mode, which must be one of the predefined valid modes.</p> <code>image</code> <code>LazyLoadingImage</code> <p>An instance of LazyLoadingImage to be processed.                                 Defaults to None.</p> <code>image_raw</code> <code>LazyLoadingImage</code> <p>An alternative raw image instance of                                     LazyLoadingImage. Defaults to None.</p> <code>strength</code> <code>float</code> <p>A float value representing the strength of the operation, must be               between 0 and 1000 (inclusive). Defaults to 1.</p> <p>Methods:</p> Name Description <code>image_raw_validate</code> <p>Validates that either 'image' or 'image_raw' is provided,                 but not both.</p> <code>mode_validate</code> <p>Validates that the 'mode' attribute is one of the predefined valid            modes in the configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised if both 'image' and 'image_raw' are specified, or if the         'mode' is not a valid mode.</p> Source code in <code>imaginairy/schema.py</code> <pre><code>class ControlInput(BaseModel):\n    \"\"\"\n    A Pydantic model representing the input control parameters for an operation,\n    typically involving image processing.\n\n    This model includes parameters such as the operation mode, the image to be processed,\n    an alternative raw image, and a strength parameter. It validates these parameters to\n    ensure they meet specific criteria, such as the mode being one of the predefined valid modes\n    and ensuring that both 'image' and 'image_raw' are not provided simultaneously.\n\n    Attributes:\n        mode (str): The operation mode, which must be one of the predefined valid modes.\n        image (LazyLoadingImage, optional): An instance of LazyLoadingImage to be processed.\n                                            Defaults to None.\n        image_raw (LazyLoadingImage, optional): An alternative raw image instance of\n                                                LazyLoadingImage. Defaults to None.\n        strength (float): A float value representing the strength of the operation, must be\n                          between 0 and 1000 (inclusive). Defaults to 1.\n\n    Methods:\n        image_raw_validate: Validates that either 'image' or 'image_raw' is provided,\n                            but not both.\n        mode_validate: Validates that the 'mode' attribute is one of the predefined valid\n                       modes in the configuration.\n\n    Raises:\n        ValueError: Raised if both 'image' and 'image_raw' are specified, or if the\n                    'mode' is not a valid mode.\n    \"\"\"\n\n    mode: str\n    image: LazyLoadingImage | None = None\n    image_raw: LazyLoadingImage | None = None\n    strength: float = Field(1, ge=0, le=1000)\n\n    # @field_validator(\"image\", \"image_raw\", mode=\"before\")\n    # def validate_images(cls, v):\n    #     if isinstance(v, str):\n    #         return LazyLoadingImage(filepath=v)\n    #\n    #     return v\n\n    @field_validator(\"image_raw\")\n    def image_raw_validate(cls, v, info: core_schema.FieldValidationInfo):\n        if info.data.get(\"image\") is not None and v is not None:\n            raise ValueError(\"You cannot specify both image and image_raw\")\n\n        # if v is None and values.get(\"image\") is None:\n        #     raise ValueError(\"You must specify either image or image_raw\")\n\n        return v\n\n    @field_validator(\"mode\")\n    def mode_validate(cls, v):\n        if v not in config.CONTROL_CONFIG_SHORTCUTS:\n            valid_modes = list(config.CONTROL_CONFIG_SHORTCUTS.keys())\n            valid_modes = \", \".join(valid_modes)\n            msg = f\"Invalid controlnet mode: '{v}'. Valid modes are: {valid_modes}\"\n            raise ValueError(msg)\n        return v\n</code></pre>"},{"location":"docs/Python/ImaginePrompt/","title":"ImaginePrompt","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>imaginairy/schema.py</code> <pre><code>class ImaginePrompt(BaseModel, protected_namespaces=()):\n    model_config = ConfigDict(extra=\"forbid\", validate_assignment=True)\n\n    prompt: List[WeightedPrompt] = Field(default=None, validate_default=True)  # type: ignore\n    negative_prompt: List[WeightedPrompt] = Field(\n        default_factory=list, validate_default=True\n    )\n    prompt_strength: float = Field(default=7.5, le=50, ge=-50, validate_default=True)\n    init_image: LazyLoadingImage | None = Field(\n        None, description=\"base64 encoded image\", validate_default=True\n    )\n    init_image_strength: float | None = Field(\n        ge=0, le=1, default=None, validate_default=True\n    )\n    image_prompt: List[LazyLoadingImage] | None = Field(None, validate_default=True)\n    image_prompt_strength: float = Field(ge=0, le=1, default=0.0)\n    control_inputs: List[ControlInput] = Field(\n        default_factory=list, validate_default=True\n    )\n    mask_prompt: str | None = Field(\n        default=None,\n        description=\"text description of the things to be masked\",\n        validate_default=True,\n    )\n    mask_image: LazyLoadingImage | None = Field(default=None, validate_default=True)\n    mask_mode: MaskMode = MaskMode.REPLACE\n    mask_modify_original: bool = True\n    outpaint: str | None = \"\"\n    model_weights: config.ModelWeightsConfig = Field(  # type: ignore\n        default=config.DEFAULT_MODEL_WEIGHTS, validate_default=True\n    )\n    solver_type: str = Field(default=config.DEFAULT_SOLVER, validate_default=True)\n    seed: int | None = Field(default=None, validate_default=True)\n    steps: int = Field(validate_default=True)\n    size: tuple[int, int] = Field(validate_default=True)\n    upscale: bool = False\n    fix_faces: bool = False\n    fix_faces_fidelity: float | None = Field(0.5, ge=0, le=1, validate_default=True)\n    conditioning: str | None = None\n    tile_mode: str = \"\"\n    allow_compose_phase: bool = True\n    is_intermediate: bool = False\n    collect_progress_latents: bool = False\n    caption_text: str = Field(\n        \"\", description=\"text to be overlaid on the image\", validate_default=True\n    )\n    composition_strength: float = Field(ge=0, le=1, validate_default=True)\n    inpaint_method: InpaintMethod = \"finetune\"\n\n    def __init__(\n        self,\n        prompt: PromptInput = \"\",\n        *,\n        negative_prompt: PromptInput = None,\n        prompt_strength: float | None = 7.5,\n        init_image: LazyLoadingImage | None = None,\n        init_image_strength: float | None = None,\n        image_prompt: LazyLoadingImage | List[LazyLoadingImage] | None = None,\n        image_prompt_strength: float | None = 0.35,\n        control_inputs: List[ControlInput] | None = None,\n        mask_prompt: str | None = None,\n        mask_image: LazyLoadingImage | None = None,\n        mask_mode: MaskInput = MaskMode.REPLACE,\n        mask_modify_original: bool = True,\n        outpaint: str | None = \"\",\n        model_weights: str | config.ModelWeightsConfig = config.DEFAULT_MODEL_WEIGHTS,\n        solver_type: str = config.DEFAULT_SOLVER,\n        seed: int | None = None,\n        steps: int | None = None,\n        size: int | str | tuple[int, int] | None = None,\n        upscale: bool = False,\n        fix_faces: bool = False,\n        fix_faces_fidelity: float | None = 0.2,\n        conditioning: str | None = None,\n        tile_mode: str = \"\",\n        allow_compose_phase: bool = True,\n        is_intermediate: bool = False,\n        collect_progress_latents: bool = False,\n        caption_text: str = \"\",\n        composition_strength: float | None = 0.5,\n        inpaint_method: InpaintMethod = \"finetune\",\n    ):\n        if image_prompt and not isinstance(image_prompt, list):\n            image_prompt = [image_prompt]\n\n        if not image_prompt_strength:\n            image_prompt_strength = 0.35\n\n        super().__init__(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            prompt_strength=prompt_strength,\n            init_image=init_image,\n            init_image_strength=init_image_strength,\n            image_prompt=image_prompt,\n            image_prompt_strength=image_prompt_strength,\n            control_inputs=control_inputs,\n            mask_prompt=mask_prompt,\n            mask_image=mask_image,\n            mask_mode=mask_mode,\n            mask_modify_original=mask_modify_original,\n            outpaint=outpaint,\n            model_weights=model_weights,\n            solver_type=solver_type,\n            seed=seed,\n            steps=steps,\n            size=size,\n            upscale=upscale,\n            fix_faces=fix_faces,\n            fix_faces_fidelity=fix_faces_fidelity,\n            conditioning=conditioning,\n            tile_mode=tile_mode,\n            allow_compose_phase=allow_compose_phase,\n            is_intermediate=is_intermediate,\n            collect_progress_latents=collect_progress_latents,\n            caption_text=caption_text,\n            composition_strength=composition_strength,\n            inpaint_method=inpaint_method,\n        )\n        self._default_negative_prompt = None\n\n    @field_validator(\"prompt\", \"negative_prompt\", mode=\"before\")\n    def make_into_weighted_prompts(\n        cls,\n        value: PromptInput,\n    ) -&gt; list[WeightedPrompt]:\n        match value:\n            case None:\n                return []\n\n            case str():\n                if value is not None:\n                    return [WeightedPrompt(text=value)]\n                else:\n                    return []\n            case WeightedPrompt():\n                return [value]\n            case list():\n                if all(isinstance(item, str) for item in value):\n                    return [WeightedPrompt(text=str(p)) for p in value]\n                elif all(isinstance(item, WeightedPrompt) for item in value):\n                    return cast(List[WeightedPrompt], value)\n        raise ValueError(\"Invalid prompt input\")\n\n    @field_validator(\"prompt\", \"negative_prompt\", mode=\"after\")\n    @classmethod\n    def must_have_some_weight(cls, v):\n        if v:\n            total_weight = sum(p.weight for p in v)\n            if total_weight == 0:\n                raise ValueError(\"Total weight of prompts cannot be 0\")\n        return v\n\n    @field_validator(\"prompt\", \"negative_prompt\", mode=\"after\")\n    def sort_prompts(cls, v):\n        if isinstance(v, list):\n            v.sort(key=lambda p: p.weight, reverse=True)\n        return v\n\n    @property\n    def default_negative_prompt(self):\n        default_negative_prompt = config.DEFAULT_NEGATIVE_PROMPT\n        if self.model_weights:\n            default_negative_prompt = self.model_weights.defaults.get(\n                \"negative_prompt\", default_negative_prompt\n            )\n        return default_negative_prompt\n\n    @model_validator(mode=\"after\")\n    def validate_negative_prompt(self):\n        if self.negative_prompt == []:\n            self.negative_prompt = [WeightedPrompt(text=self.default_negative_prompt)]\n\n        return self\n\n    @field_validator(\"prompt_strength\", mode=\"before\")\n    def validate_prompt_strength(cls, v):\n        return 7.5 if v is None else v\n\n    @field_validator(\"tile_mode\", mode=\"before\")\n    def validate_tile_mode(cls, v):\n        valid_tile_modes = (\"\", \"x\", \"y\", \"xy\")\n        if v is True:\n            return \"xy\"\n\n        if v is False or v is None:\n            return \"\"\n\n        if not isinstance(v, str):\n            msg = f\"Invalid tile_mode: '{v}'. Valid modes are: {valid_tile_modes}\"\n            raise ValueError(msg)  # noqa\n\n        v = v.lower()\n        if v not in valid_tile_modes:\n            msg = f\"Invalid tile_mode: '{v}'. Valid modes are: {valid_tile_modes}\"\n            raise ValueError(msg)\n        return v\n\n    @field_validator(\"outpaint\", mode=\"after\")\n    def validate_outpaint(cls, v):\n        from imaginairy.utils.outpaint import outpaint_arg_str_parse\n\n        outpaint_arg_str_parse(v)\n        return v\n\n    @field_validator(\"conditioning\", mode=\"after\")\n    def validate_conditioning(cls, v):\n        from torch import Tensor\n\n        if v is None:\n            return v\n\n        if not isinstance(v, Tensor):\n            raise ValueError(\"conditioning must be a torch.Tensor\")  # noqa\n        return v\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def set_default_composition_strength(cls, data: Any) -&gt; Any:\n        if not isinstance(data, dict):\n            return data\n        comp_strength = data.get(\"composition_strength\")\n        default_comp_strength = 0.5\n        if comp_strength is None:\n            model_weights = data.get(\"model_weights\")\n            if isinstance(model_weights, config.ModelWeightsConfig):\n                default_comp_strength = model_weights.defaults.get(\n                    \"composition_strength\", default_comp_strength\n                )\n            data[\"composition_strength\"] = default_comp_strength\n\n        return data\n\n    # @field_validator(\"init_image\", \"mask_image\", mode=\"after\")\n    # def handle_images(cls, v):\n    #     if isinstance(v, str):\n    #         return LazyLoadingImage(filepath=v)\n    #\n    #     return v\n\n    @model_validator(mode=\"after\")\n    def set_init_from_control_inputs(self):\n        if self.init_image is None:\n            for control_input in self.control_inputs:\n                if control_input.image:\n                    self.init_image = control_input.image\n                    break\n\n        return self\n\n    @field_validator(\"control_inputs\", mode=\"before\")\n    def validate_control_inputs(cls, v):\n        if v is None:\n            v = []\n        return v\n\n    @field_validator(\"control_inputs\", mode=\"after\")\n    def set_image_from_init_image(cls, v, info: core_schema.FieldValidationInfo):\n        v = v or []\n        for control_input in v:\n            if control_input.image is None and control_input.image_raw is None:\n                control_input.image = info.data[\"init_image\"]\n        return v\n\n    @field_validator(\"mask_image\")\n    def validate_mask_image(cls, v, info: core_schema.FieldValidationInfo):\n        if v is not None and info.data.get(\"mask_prompt\") is not None:\n            msg = \"You can only set one of `mask_image` and `mask_prompt`\"\n            raise ValueError(msg)\n        return v\n\n    @field_validator(\"mask_prompt\", \"mask_image\", mode=\"before\")\n    def validate_mask_prompt(cls, v, info: core_schema.FieldValidationInfo):\n        if info.data.get(\"init_image\") is None and v:\n            msg = \"You must set `init_image` if you want to use a mask\"\n            raise ValueError(msg)\n        return v\n\n    @model_validator(mode=\"before\")\n    def resolve_model_weights(cls, data: Any):\n        if not isinstance(data, dict):\n            return data\n\n        model_weights = data.get(\"model_weights\")\n        if model_weights is None:\n            model_weights = config.DEFAULT_MODEL_WEIGHTS\n        from imaginairy.utils.model_manager import resolve_model_weights_config\n\n        should_use_inpainting = bool(\n            data.get(\"mask_image\") or data.get(\"mask_prompt\") or data.get(\"outpaint\")\n        )\n        should_use_inpainting_weights = (\n            should_use_inpainting and data.get(\"inpaint_method\") == \"finetune\"\n        )\n        model_weights_config = resolve_model_weights_config(\n            model_weights=model_weights,\n            default_model_architecture=None,\n            for_inpainting=should_use_inpainting_weights,\n        )\n        data[\"model_weights\"] = model_weights_config\n\n        return data\n\n    @field_validator(\"seed\")\n    def validate_seed(cls, v):\n        return v\n\n    @field_validator(\"fix_faces_fidelity\", mode=\"before\")\n    def validate_fix_faces_fidelity(cls, v):\n        if v is None:\n            return 0.5\n\n        return v\n\n    @field_validator(\"solver_type\", mode=\"after\")\n    def validate_solver_type(cls, v, info: core_schema.FieldValidationInfo):\n        from imaginairy.samplers import SolverName\n\n        if v is None:\n            v = config.DEFAULT_SOLVER\n\n        v = v.lower()\n\n        if info.data.get(\"model\") == \"edit\" and v in (\n            SolverName.PLMS,\n            SolverName.DDIM,\n        ):\n            msg = \"PLMS and DDIM solvers are not supported for pix2pix edit model.\"\n            raise ValueError(msg)\n        return v\n\n    @field_validator(\"steps\", mode=\"before\")\n    def validate_steps(cls, v, info: core_schema.FieldValidationInfo):\n        model_weights = info.data.get(\"model_weights\")\n\n        # Try to get steps from model weights defaults\n        if (\n            v is None\n            and model_weights\n            and isinstance(model_weights, config.ModelWeightsConfig)\n        ):\n            v = model_weights.defaults.get(\"steps\")\n\n        # If not found in model weights, try model architecture defaults\n        if v is None and model_weights and model_weights.architecture:\n            v = model_weights.architecture.defaults.get(\"steps\")\n\n        # If still not found, use solver-specific defaults\n        if v is None:\n            solver_type = info.data.get(\"solver_type\", \"ddim\").lower()\n            steps_lookup = {\"ddim\": 50, \"dpmpp\": 20}\n            v = steps_lookup.get(\n                solver_type, 50\n            )  # Default to 50 if solver not recognized\n\n        try:\n            return int(v)\n        except (OverflowError, TypeError) as e:\n            raise ValueError(\"Steps must be an integer\") from e\n\n    @model_validator(mode=\"after\")\n    def validate_init_image_strength(self):\n        if self.init_image_strength is None:\n            if self.control_inputs:\n                self.init_image_strength = 0.0\n            elif self.outpaint or self.mask_image or self.mask_prompt:\n                self.init_image_strength = 0.0\n            else:\n                self.init_image_strength = 0.2\n\n        return self\n\n    @field_validator(\"size\", mode=\"before\")\n    def validate_image_size(cls, v, info: core_schema.FieldValidationInfo):\n        from imaginairy.utils.model_manager import get_model_default_image_size\n        from imaginairy.utils.named_resolutions import normalize_image_size\n\n        if v is None:\n            v = get_model_default_image_size(info.data[\"model_weights\"].architecture)\n\n        width, height = normalize_image_size(v)\n\n        return width, height\n\n    @field_validator(\"size\", mode=\"after\")\n    def validate_image_size_after(cls, v, info: core_schema.FieldValidationInfo):\n        width, height = v\n        min_size = 8\n        max_size = 100_000\n        if not min_size &lt;= width &lt;= max_size:\n            msg = f\"Width must be between {min_size} and {max_size}. Got: {width}\"\n            raise ValueError(msg)\n\n        if not min_size &lt;= height &lt;= max_size:\n            msg = f\"Height must be between {min_size} and {max_size}. Got: {height}\"\n            raise ValueError(msg)\n        return v\n\n    @field_validator(\"caption_text\", mode=\"before\")\n    def validate_caption_text(cls, v):\n        if v is None:\n            v = \"\"\n\n        return v\n\n    @property\n    def prompts(self):\n        return self.prompt\n\n    @property\n    def prompt_text(self) -&gt; str:\n        if not self.prompt:\n            return \"\"\n        if len(self.prompt) == 1:\n            return self.prompt[0].text\n        return \"|\".join(str(p) for p in self.prompt)\n\n    @property\n    def negative_prompt_text(self) -&gt; str:\n        if not self.negative_prompt:\n            return \"\"\n        if len(self.negative_prompt) == 1:\n            return self.negative_prompt[0].text\n        return \"|\".join(str(p) for p in self.negative_prompt)\n\n    @property\n    def width(self) -&gt; int:\n        return self.size[0]\n\n    @property\n    def height(self) -&gt; int:\n        return self.size[1]\n\n    @property\n    def aspect_ratio(self) -&gt; str:\n        from imaginairy.utils.img_utils import aspect_ratio\n\n        return aspect_ratio(width=self.width, height=self.height)\n\n    @property\n    def should_use_inpainting(self) -&gt; bool:\n        return bool(self.outpaint or self.mask_image or self.mask_prompt)\n\n    @property\n    def should_use_inpainting_weights(self) -&gt; bool:\n        return self.should_use_inpainting and self.inpaint_method == \"finetune\"\n\n    @property\n    def model_architecture(self) -&gt; config.ModelArchitecture:\n        return self.model_weights.architecture\n\n    def prompt_description(self):\n        if self.negative_prompt_text == self.default_negative_prompt:\n            neg_prompt = \"DEFAULT-NEGATIVE-PROMPT\"\n        else:\n            neg_prompt = f'\"{self.negative_prompt_text}\"'\n\n        from termcolor import colored\n\n        prompt_text = colored(self.prompt_text, \"green\")\n\n        return (\n            f'\"{prompt_text}\"\\n'\n            \"    \"\n            f\"negative-prompt:{neg_prompt}\\n\"\n            \"    \"\n            f\"size:{self.width}x{self.height}px-({self.aspect_ratio}) \"\n            f\"seed:{self.seed} \"\n            f\"prompt-strength:{self.prompt_strength} \"\n            f\"steps:{self.steps} solver-type:{self.solver_type} \"\n            f\"init-image-strength:{self.init_image_strength} \"\n            f\"arch:{self.model_architecture.aliases[0]} \"\n            f\"weights:{self.model_weights.aliases[0]}\"\n        )\n\n    def logging_dict(self):\n        \"\"\"Return a dict of the object but with binary data replaced with reprs.\"\"\"\n        data = self.model_dump()\n        data[\"init_image\"] = repr(self.init_image)\n        data[\"mask_image\"] = repr(self.mask_image)\n        data[\"image_prompt\"] = repr(self.image_prompt)\n        if self.control_inputs:\n            data[\"control_inputs\"] = [repr(ci) for ci in self.control_inputs]\n        return data\n\n    def full_copy(self, deep=True, update=None):\n        new_prompt = self.model_copy(\n            deep=deep,\n            update=update,\n        )\n        # new_prompt = self.model_validate(new_prompt) doesn't work for some reason https://github.com/pydantic/pydantic/issues/7387\n        new_prompt = new_prompt.model_validate(dict(new_prompt))\n        return new_prompt\n\n    def make_concrete_copy(self) -&gt; Self:\n        seed = self.seed if self.seed is not None else random.randint(1, 1_000_000_000)\n        return self.full_copy(\n            deep=False,\n            update={\n                \"seed\": seed,\n            },\n        )\n</code></pre>"},{"location":"docs/Python/ImaginePrompt/#imaginairy.schema.ImaginePrompt.logging_dict","title":"<code>logging_dict()</code>","text":"<p>Return a dict of the object but with binary data replaced with reprs.</p> Source code in <code>imaginairy/schema.py</code> <pre><code>def logging_dict(self):\n    \"\"\"Return a dict of the object but with binary data replaced with reprs.\"\"\"\n    data = self.model_dump()\n    data[\"init_image\"] = repr(self.init_image)\n    data[\"mask_image\"] = repr(self.mask_image)\n    data[\"image_prompt\"] = repr(self.image_prompt)\n    if self.control_inputs:\n        data[\"control_inputs\"] = [repr(ci) for ci in self.control_inputs]\n    return data\n</code></pre>"},{"location":"docs/Python/LazyLoadingImage/","title":"LazyLoadingImage","text":"<p>A class representing an image that can be lazily loaded from various sources.</p> <p>This class supports loading an image from a filepath, URL, a PIL Image object, or a base64 encoded string. The image is only loaded into memory when it's accessed, not at the time of object creation. If multiple sources are provided, an error is raised. The class also provides functionality to convert the image to a base64 string and to access it as a PIL Image object.</p> <p>Attributes:</p> Name Type Description <code>_lazy_filepath</code> <code>str</code> <p>Path to the image file, if provided.</p> <code>_lazy_url</code> <code>str</code> <p>URL of the image, if provided.</p> <code>_img</code> <code>Image</code> <p>PIL Image object, if provided.</p> <p>Methods:</p> Name Description <code>_load_img</code> <p>Lazily loads the image from the specified source.</p> <code>as_base64</code> <p>Returns the image encoded as a base64 string.</p> <code>as_pillow</code> <p>Returns the image as a PIL Image object.</p> <code>save_image_as_base64</code> <p>Static method to convert a PIL Image to a base64 string.</p> <code>load_image_from_base64</code> <p>Static method to load an image from a base64 string.</p> <code>__get_pydantic_core_schema__</code> <p>Class method for Pydantic schema generation.</p> Source code in <code>imaginairy/schema.py</code> <pre><code>class LazyLoadingImage:\n    \"\"\"\n    A class representing an image that can be lazily loaded from various sources.\n\n    This class supports loading an image from a filepath, URL, a PIL Image object,\n    or a base64 encoded string. The image is only loaded into memory when it's\n    accessed, not at the time of object creation. If multiple sources are provided,\n    an error is raised. The class also provides functionality to convert the image\n    to a base64 string and to access it as a PIL Image object.\n\n    Attributes:\n        _lazy_filepath (str): Path to the image file, if provided.\n        _lazy_url (str): URL of the image, if provided.\n        _img (Image.Image): PIL Image object, if provided.\n\n    Methods:\n        _load_img: Lazily loads the image from the specified source.\n        as_base64: Returns the image encoded as a base64 string.\n        as_pillow: Returns the image as a PIL Image object.\n        save_image_as_base64: Static method to convert a PIL Image to a base64 string.\n        load_image_from_base64: Static method to load an image from a base64 string.\n        __get_pydantic_core_schema__: Class method for Pydantic schema generation.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        filepath: str | None = None,\n        url: str | None = None,\n        img: \"Image.Image | None\" = None,\n        b64: str | None = None,\n    ):\n        if not filepath and not url and not img and not b64:\n            msg = \"You must specify a url or filepath or img or base64 string\"\n            raise ValueError(msg)\n        if sum([bool(filepath), bool(url), bool(img), bool(b64)]) &gt; 1:\n            raise ValueError(\"You cannot multiple input methods\")\n\n        # validate file exists\n        if filepath and not os.path.exists(filepath):\n            msg = f\"File does not exist: {filepath}\"\n            raise FileNotFoundError(msg)\n\n        # validate url is valid url\n        if url:\n            from urllib3.exceptions import LocationParseError\n            from urllib3.util import parse_url\n\n            try:\n                parsed_url = parse_url(url)\n            except LocationParseError:\n                raise InvalidUrlError(f\"Invalid url: {url}\")  # noqa\n            if parsed_url.scheme not in {\"http\", \"https\"} or not parsed_url.host:\n                msg = f\"Invalid url: {url}\"\n                raise InvalidUrlError(msg)\n\n        if b64:\n            img = self.load_image_from_base64(b64)\n\n        self._lazy_filepath = filepath\n        self._lazy_url = url\n        self._img = img\n\n    def __getattr__(self, key):\n        if key == \"_img\":\n            #  http://nedbatchelder.com/blog/201010/surprising_getattr_recursion.html\n            raise AttributeError()\n        self._load_img()\n        return getattr(self._img, key)\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n\n    def __getstate__(self):\n        return self.__dict__\n\n    def _load_img(self):\n        if self._img is None:\n            from PIL import Image, ImageOps\n\n            if self._lazy_filepath:\n                self._img = Image.open(self._lazy_filepath)\n                logger.debug(\n                    f\"Loaded input \ud83d\uddbc  of size {self._img.size} from {self._lazy_filepath}\"\n                )\n            elif self._lazy_url:\n                import requests\n\n                self._img = Image.open(\n                    BytesIO(\n                        requests.get(self._lazy_url, stream=True, timeout=60).content\n                    )\n                )\n\n                logger.debug(\n                    f\"Loaded input \ud83d\uddbc  of size {self._img.size} from {self._lazy_url}\"\n                )\n            else:\n                raise ValueError(\"You must specify a url or filepath\")\n            # fix orientation\n            self._img = ImageOps.exif_transpose(self._img)\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source_type: Any, handler: GetCoreSchemaHandler\n    ) -&gt; core_schema.CoreSchema:\n        def validate(value: Any) -&gt; \"LazyLoadingImage\":\n            from PIL import Image, UnidentifiedImageError\n\n            if isinstance(value, cls):\n                return value\n            if isinstance(value, Image.Image):\n                return cls(img=value)\n            if isinstance(value, str):\n                if \".\" in value[:1000]:\n                    try:\n                        return cls(filepath=value)\n                    except FileNotFoundError as e:\n                        raise ValueError(str(e))  # noqa\n                try:\n                    return cls(b64=value)\n                except UnidentifiedImageError:\n                    msg = \"base64 string was not recognized as a valid image type\"\n                    raise ValueError(msg)  # noqa\n            if isinstance(value, dict):\n                return cls(**value)\n            msg = \"Image value must be either a LazyLoadingImage, PIL.Image.Image or a Base64 string\"\n            raise ValueError(msg)\n\n        def handle_b64(value: Any) -&gt; \"LazyLoadingImage\":\n            if isinstance(value, str):\n                return cls(b64=value)\n            msg = \"Image value must be either a LazyLoadingImage, PIL.Image.Image or a Base64 string\"\n            raise ValueError(msg)\n\n        return core_schema.json_or_python_schema(\n            json_schema=core_schema.chain_schema(\n                [\n                    core_schema.str_schema(),\n                    core_schema.no_info_before_validator_function(\n                        handle_b64, core_schema.any_schema()\n                    ),\n                ]\n            ),\n            python_schema=core_schema.no_info_before_validator_function(\n                validate, core_schema.any_schema()\n            ),\n            serialization=core_schema.plain_serializer_function_ser_schema(str),\n        )\n\n    @staticmethod\n    def save_image_as_base64(image: \"Image.Image\") -&gt; str:\n        buffered = io.BytesIO()\n        image.save(buffered, format=\"PNG\")\n        img_bytes = buffered.getvalue()\n        return base64.b64encode(img_bytes).decode()\n\n    @staticmethod\n    def load_image_from_base64(image_str: str) -&gt; \"Image.Image\":\n        from PIL import Image\n\n        img_bytes = base64.b64decode(image_str)\n        return Image.open(io.BytesIO(img_bytes))\n\n    def as_base64(self):\n        self._load_img()\n        return self.save_image_as_base64(self._img)\n\n    def as_pillow(self):\n        self._load_img()\n        return self._img\n\n    def __str__(self):\n        return self.as_base64()\n\n    def __repr__(self):\n        \"\"\"human readable representation.\n\n        shows filepath or url if available.\n        \"\"\"\n        try:\n            return f\"&lt;LazyLoadingImage filepath={self._lazy_filepath} url={self._lazy_url}&gt;\"\n        except Exception as e:  # noqa\n            return f\"&lt;LazyLoadingImage RENDER EXCEPTION*{e}*&gt;\"\n</code></pre>"},{"location":"docs/Python/LazyLoadingImage/#imaginairy.schema.LazyLoadingImage.__repr__","title":"<code>__repr__()</code>","text":"<p>human readable representation.</p> <p>shows filepath or url if available.</p> Source code in <code>imaginairy/schema.py</code> <pre><code>def __repr__(self):\n    \"\"\"human readable representation.\n\n    shows filepath or url if available.\n    \"\"\"\n    try:\n        return f\"&lt;LazyLoadingImage filepath={self._lazy_filepath} url={self._lazy_url}&gt;\"\n    except Exception as e:  # noqa\n        return f\"&lt;LazyLoadingImage RENDER EXCEPTION*{e}*&gt;\"\n</code></pre>"},{"location":"docs/Python/WeightedPrompt/","title":"WeightedPrompt","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a prompt with an associated weight.</p> <p>This class is used to define a text prompt with a corresponding numerical weight, indicating the significance or influence of the prompt in a given context, such as in image generation or text processing tasks.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>The textual content of the prompt.</p> <code>weight</code> <code>float</code> <p>A numerical weight associated with the prompt. Defaults to 1.             The weight must be greater than or equal to 0.</p> <p>Methods:</p> Name Description <code>__repr__</code> <p>Returns a string representation of the WeightedPrompt instance,       formatted as 'weight*(text)'.</p> Source code in <code>imaginairy/schema.py</code> <pre><code>class WeightedPrompt(BaseModel):\n    \"\"\"\n    Represents a prompt with an associated weight.\n\n    This class is used to define a text prompt with a corresponding numerical weight,\n    indicating the significance or influence of the prompt in a given context, such as\n    in image generation or text processing tasks.\n\n    Attributes:\n        text (str): The textual content of the prompt.\n        weight (float): A numerical weight associated with the prompt. Defaults to 1.\n                        The weight must be greater than or equal to 0.\n\n    Methods:\n        __repr__: Returns a string representation of the WeightedPrompt instance,\n                  formatted as 'weight*(text)'.\n    \"\"\"\n\n    text: str\n    weight: float = Field(1, ge=0)\n\n    def __repr__(self):\n        return f\"{self.weight}*({self.text})\"\n</code></pre>"},{"location":"docs/Python/colorize-img/","title":"colorize_img()","text":"<p>Colorizes black and white images or re-colors existing images. Optionally adds a caption.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Image</code> <p>The image to be colorized.</p> required <code>max_width</code> <code>int</code> <p>The maximum width of the output image, defaults to 1024.</p> <code>1024</code> <code>max_height</code> <code>int</code> <p>The maximum height of the output image, defaults to 1024.</p> <code>1024</code> <code>caption</code> <code>str</code> <p>A caption for the image. If None, a caption is generated and modified.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Image</code> <p>The colorized version of the input image.</p> Source code in <code>imaginairy/api/colorize.py</code> <pre><code>def colorize_img(img, max_width=1024, max_height=1024, caption=None):\n    \"\"\"\n    Colorizes black and white images or re-colors existing images. Optionally adds a caption.\n\n    Args:\n        img (Image): The image to be colorized.\n        max_width (int, optional): The maximum width of the output image, defaults to 1024.\n        max_height (int, optional): The maximum height of the output image, defaults to 1024.\n        caption (str, optional): A caption for the image. If None, a caption is generated and modified.\n\n    Returns:\n        Image: The colorized version of the input image.\n\n    \"\"\"\n    if not caption:\n        caption = generate_caption(img, min_length=10)\n        caption = caption.replace(\"black and white\", \"color\")\n        caption = caption.replace(\"old picture\", \"professional color photo\")\n        caption = caption.replace(\"vintage photograph\", \"professional color photo\")\n        caption = caption.replace(\"old photo\", \"professional color photo\")\n        caption = caption.replace(\"vintage photo\", \"professional color photo\")\n        caption = caption.replace(\"old color\", \"color\")\n        caption = caption.replace(\" old fashioned \", \" \")\n        caption = caption.replace(\" old time \", \" \")\n        caption = caption.replace(\" old \", \" \")\n        logger.info(caption)\n    control_inputs = [\n        ControlInput(mode=\"colorize\", image=img, strength=2),\n    ]\n    prompt_add = \". color photo, sharp-focus, highly detailed, intricate, Canon 5D\"\n    prompt = ImaginePrompt(\n        prompt=f\"{caption}{prompt_add}\",\n        init_image=img,\n        init_image_strength=0.0,\n        control_inputs=control_inputs,\n        size=(min(img.width, max_width), min(img.height, max_height)),\n        steps=30,\n        prompt_strength=12,\n    )\n    result = next(iter(imagine(prompt)))\n    colorized_img = replace_color(img, result.images[\"generated\"])\n\n    # allows the algorithm some leeway for the overall brightness of the image\n    # results look better with this\n    colorized_img = match_brightness(colorized_img, result.images[\"generated\"])\n\n    return colorized_img\n</code></pre>"},{"location":"docs/Python/generate-video/","title":"generate_video()","text":"<p>Generates a video from a single image or multiple images, conditioned on the provided input_path.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to an image file or a directory containing image files.</p> required <code>output_folder</code> <code>str | None</code> <p>Directory where the generated video will be saved. Defaults to \"outputs/video/\" if None.</p> <code>None</code> <code>num_frames</code> <code>int</code> <p>Number of frames in the generated video. Defaults to 6.</p> <code>6</code> <code>num_steps</code> <code>int</code> <p>Number of steps for the generation process. Defaults to 30.</p> <code>30</code> <code>model_name</code> <code>str</code> <p>Name of the model to use for generation. Defaults to \"svd_xt\".</p> <code>'svd-xt'</code> <code>fps_id</code> <code>int</code> <p>Frame rate identifier used in generation. Defaults to 6.</p> <code>6</code> <code>output_fps</code> <code>int</code> <p>Frame rate of the output video. Defaults to 6.</p> <code>6</code> <code>motion_bucket_id</code> <code>int</code> <p>Identifier for motion bucket. Defaults to 127.</p> <code>127</code> <code>cond_aug</code> <code>float</code> <p>Conditional augmentation value. Defaults to 0.02.</p> <code>0.02</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for generation. If None, a random seed is chosen.</p> <code>None</code> <code>decoding_t</code> <code>int</code> <p>Number of frames decoded at a time, affecting VRAM usage. Reduce if necessary. Defaults to 1.</p> <code>1</code> <code>device</code> <code>Optional[str]</code> <p>Device to run the generation on. Defaults to the detected device.</p> <code>None</code> <code>repetitions</code> <code>int</code> <p>Number of times to repeat the video generation process. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>None</code> <p>The function saves the generated video(s) to the specified output folder.</p> Source code in <code>imaginairy/api/video_sample.py</code> <pre><code>def generate_video(\n    input_path: str,  # Can either be image file or folder with image files\n    output_folder: str | None = None,\n    size=(1024, 576),\n    num_frames: int = 6,\n    num_steps: int = 30,\n    model_name: str = \"svd-xt\",\n    fps_id: int = 6,\n    output_fps: int = 6,\n    motion_bucket_id: int = 127,\n    cond_aug: float = 0.02,\n    seed: Optional[int] = None,\n    decoding_t: int = 1,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n    device: Optional[str] = None,\n    repetitions=1,\n    output_format=\"webp\",\n):\n    \"\"\"\n    Generates a video from a single image or multiple images, conditioned on the provided input_path.\n\n    Args:\n        input_path (str): Path to an image file or a directory containing image files.\n        output_folder (str | None, optional): Directory where the generated video will be saved.\n            Defaults to \"outputs/video/\" if None.\n        num_frames (int, optional): Number of frames in the generated video. Defaults to 6.\n        num_steps (int, optional): Number of steps for the generation process. Defaults to 30.\n        model_name (str, optional): Name of the model to use for generation. Defaults to \"svd_xt\".\n        fps_id (int, optional): Frame rate identifier used in generation. Defaults to 6.\n        output_fps (int, optional): Frame rate of the output video. Defaults to 6.\n        motion_bucket_id (int, optional): Identifier for motion bucket. Defaults to 127.\n        cond_aug (float, optional): Conditional augmentation value. Defaults to 0.02.\n        seed (Optional[int], optional): Random seed for generation. If None, a random seed is chosen.\n        decoding_t (int, optional): Number of frames decoded at a time, affecting VRAM usage.\n            Reduce if necessary. Defaults to 1.\n        device (Optional[str], optional): Device to run the generation on. Defaults to the detected device.\n        repetitions (int, optional): Number of times to repeat the video generation process. Defaults to 1.\n\n    Returns:\n        None: The function saves the generated video(s) to the specified output folder.\n    \"\"\"\n    device = default(device, get_device)\n    vid_width, vid_height = normalize_image_size(size)\n    if device == \"mps\":\n        msg = \"Apple Silicon MPS (M1, M2, etc) is not currently supported for video generation. Switching to cpu generation.\"\n        logger.warning(msg)\n        device = \"cpu\"\n\n    elif not torch.cuda.is_available():\n        msg = (\n            \"CUDA is not available. This will be verrrry slow or not work at all.\\n\"\n            \"If you have a GPU, make sure you have CUDA installed and PyTorch is compiled with CUDA support.\\n\"\n            \"Unfortunately, we cannot automatically install the proper version.\\n\\n\"\n            \"You can install the proper version by following these directions:\\n\"\n            \"https://pytorch.org/get-started/locally/\"\n        )\n        logger.warning(msg)\n\n    output_fps = default(output_fps, fps_id)\n\n    model_name = model_name.lower().replace(\"_\", \"-\")\n\n    video_model_config = config.MODEL_WEIGHT_CONFIG_LOOKUP.get(model_name, None)\n    if video_model_config is None:\n        msg = f\"Version {model_name} does not exist.\"\n        raise ValueError(msg)\n\n    num_frames = default(num_frames, video_model_config.defaults.get(\"frames\", 12))\n    num_steps = default(num_steps, video_model_config.defaults.get(\"steps\", 30))\n    output_folder_str = default(output_folder, \"outputs/video/\")\n    del output_folder\n    video_config_path = f\"{PKG_ROOT}/{video_model_config.architecture.config_path}\"\n\n    model, safety_filter = load_model(\n        config=video_config_path,\n        device=\"cpu\",\n        num_frames=num_frames,\n        num_steps=num_steps,\n        weights_url=video_model_config.weights_location,\n    )\n\n    if input_path.startswith(\"http\"):\n        all_img_paths = [input_path]\n    else:\n        path = Path(input_path)\n        if path.is_file():\n            if any(input_path.endswith(x) for x in [\"jpg\", \"jpeg\", \"png\"]):\n                all_img_paths = [input_path]\n            else:\n                raise ValueError(\"Path is not valid image file.\")\n        elif path.is_dir():\n            all_img_paths = sorted(\n                [\n                    str(f)\n                    for f in path.iterdir()\n                    if f.is_file() and f.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n                ]\n            )\n            if len(all_img_paths) == 0:\n                raise ValueError(\"Folder does not contain any images.\")\n        else:\n            msg = f\"Could not find file or folder at {input_path}\"\n            raise FileNotFoundError(msg)\n\n    expected_size = (vid_width, vid_height)\n    for _ in range(repetitions):\n        for input_path in all_img_paths:\n            start_time = time.perf_counter()\n            _seed = default(seed, random.randint(0, 1000000))\n            torch.manual_seed(_seed)\n            logger.info(\n                f\"Generating a {num_frames} frame video from {input_path}. Device:{device} seed:{_seed}\"\n            )\n            if input_path.startswith(\"http\"):\n                image = LazyLoadingImage(url=input_path).as_pillow()\n            else:\n                image = LazyLoadingImage(filepath=input_path).as_pillow()\n            crop_coords = None\n            if image.mode == \"RGBA\":\n                image = image.convert(\"RGB\")\n            if image.size != expected_size:\n                logger.info(\n                    f\"Resizing image from {image.size} to {expected_size}. (w, h)\"\n                )\n                image = pillow_fit_image_within(\n                    image, max_height=expected_size[1], max_width=expected_size[0]\n                )\n                logger.debug(f\"Image is now of size: {image.size}\")\n                background = Image.new(\"RGB\", expected_size, \"white\")\n                # Calculate the position to center the original image\n                x = (background.width - image.width) // 2\n                y = (background.height - image.height) // 2\n                background.paste(image, (x, y))\n                # crop_coords = (x, y, x + image.width, y + image.height)\n\n                # image = background\n            w, h = image.size\n            snap_to = 64\n            if h % snap_to != 0 or w % snap_to != 0:\n                width = w - w % snap_to\n                height = h - h % snap_to\n                image = image.resize((width, height))\n                logger.warning(\n                    f\"Your image is of size {h}x{w} which is not divisible by 64. We are resizing to {height}x{width}!\"\n                )\n\n            image = ToTensor()(image)\n            image = image * 2.0 - 1.0\n\n            image = image.unsqueeze(0).to(device)\n            H, W = image.shape[2:]\n            assert image.shape[1] == 3\n            F = 8\n            C = 4\n            shape = (num_frames, C, H // F, W // F)\n            if expected_size != (W, H):\n                logger.warning(\n                    f\"The {W, H} image you provided is not {expected_size}.  This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\"\n                )\n            if motion_bucket_id &gt; 255:\n                logger.warning(\n                    \"High motion bucket! This may lead to suboptimal performance.\"\n                )\n\n            if fps_id &lt; 5:\n                logger.warning(\n                    \"Small fps value! This may lead to suboptimal performance.\"\n                )\n\n            if fps_id &gt; 30:\n                logger.warning(\n                    \"Large fps value! This may lead to suboptimal performance.\"\n                )\n\n            value_dict: dict[str, Any] = {}\n            value_dict[\"motion_bucket_id\"] = motion_bucket_id\n            value_dict[\"fps_id\"] = fps_id\n            value_dict[\"cond_aug\"] = cond_aug\n            value_dict[\"cond_frames_without_noise\"] = image\n            value_dict[\"cond_frames\"] = image + cond_aug * torch.randn_like(image)\n\n            with torch.no_grad(), platform_appropriate_autocast():\n                reload_model(model.conditioner, device=device)\n                if device == \"cpu\":\n                    model.conditioner.to(torch.float32)\n                for k in value_dict:\n                    if isinstance(value_dict[k], torch.Tensor):\n                        value_dict[k] = value_dict[k].to(\n                            next(model.conditioner.parameters()).dtype\n                        )\n                batch, batch_uc = get_batch(\n                    get_unique_embedder_keys_from_conditioner(model.conditioner),\n                    value_dict,\n                    [1, num_frames],\n                    T=num_frames,\n                    device=device,\n                )\n                c, uc = model.conditioner.get_unconditional_conditioning(\n                    batch,\n                    batch_uc=batch_uc,\n                    force_uc_zero_embeddings=[\n                        \"cond_frames\",\n                        \"cond_frames_without_noise\",\n                    ],\n                )\n                unload_model(model.conditioner)\n\n                for k in [\"crossattn\", \"concat\"]:\n                    uc[k] = repeat(uc[k], \"b ... -&gt; b t ...\", t=num_frames)\n                    uc[k] = rearrange(uc[k], \"b t ... -&gt; (b t) ...\", t=num_frames)\n                    c[k] = repeat(c[k], \"b ... -&gt; b t ...\", t=num_frames)\n                    c[k] = rearrange(c[k], \"b t ... -&gt; (b t) ...\", t=num_frames)\n\n                randn = torch.randn(shape, device=device, dtype=torch.float16)\n\n                additional_model_inputs = {}\n                additional_model_inputs[\"image_only_indicator\"] = torch.zeros(\n                    2, num_frames\n                ).to(device)\n                additional_model_inputs[\"num_video_frames\"] = batch[\"num_video_frames\"]\n\n                def denoiser(_input, sigma, c):\n                    _input = _input.half().to(device)\n                    return model.denoiser(\n                        model.model, _input, sigma, c, **additional_model_inputs\n                    )\n\n                reload_model(model.denoiser, device=device)\n                reload_model(model.model, device=device)\n                samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n                unload_model(model.model)\n                unload_model(model.denoiser)\n\n                reload_model(model.first_stage_model, device=device)\n                model.en_and_decode_n_samples_a_time = decoding_t\n                samples_x = model.decode_first_stage(samples_z)\n                samples = torch.clamp((samples_x + 1.0) / 2.0, min=0.0, max=1.0)\n                unload_model(model.first_stage_model)\n\n                if crop_coords:\n                    left, upper, right, lower = crop_coords\n                    samples = samples[:, :, upper:lower, left:right]\n\n                os.makedirs(output_folder_str, exist_ok=True)\n                base_count = len(glob(os.path.join(output_folder_str, \"*.*\"))) + 1\n                source_slug = make_safe_filename(input_path)\n                video_filename = f\"{base_count:06d}_{model_name}_{_seed}_{fps_id}fps_{source_slug}.{output_format}\"\n                video_path = os.path.join(output_folder_str, video_filename)\n\n                samples = safety_filter(samples)\n                # save_video(samples, video_path, output_fps)\n                save_video_bounce(samples, video_path, output_fps)\n\n            duration = time.perf_counter() - start_time\n            logger.info(\n                f\"Video of {num_frames} frames generated in {duration:.2f} seconds and saved to {video_path}\\n\"\n            )\n</code></pre>"},{"location":"docs/Python/imagine-image-files/","title":"imagine_image_files()","text":"<p>Generates and saves image files based on given prompts, with options for animations and videos.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[ImaginePrompt] | ImaginePrompt</code> <p>A prompt or list of prompts for image generation.</p> required <code>outdir</code> <code>str</code> <p>Directory path where the generated images and other files will be saved.</p> required <code>precision</code> <code>str</code> <p>Precision mode for image generation, defaults to 'autocast'.</p> <code>'autocast'</code> <code>record_step_images</code> <code>bool</code> <p>If True, saves step-by-step images of the generation process, defaults to False.</p> <code>False</code> <code>output_file_extension</code> <code>str</code> <p>File extension for output images, must be 'jpg' or 'png', defaults to 'jpg'.</p> <code>'jpg'</code> <code>print_caption</code> <code>bool</code> <p>If True, prints captions on the generated images, defaults to False.</p> <code>False</code> <code>make_gif</code> <code>bool</code> <p>If True, creates a GIF from the generation steps, defaults to False.</p> <code>False</code> <code>make_compare_gif</code> <code>bool</code> <p>If True, creates a comparison GIF with initial and generated images, defaults to False.</p> <code>False</code> <code>return_filename_type</code> <code>str</code> <p>Type of filenames to return, defaults to 'generated'.</p> <code>'generated'</code> <code>videogen</code> <code>bool</code> <p>If True, generates a video from the generated images, defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>list[str]: A list of filenames of the generated images.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the output file extension is not 'jpg' or 'png'.</p> Source code in <code>imaginairy/api/generate.py</code> <pre><code>def imagine_image_files(\n    prompts: \"list[ImaginePrompt] | ImaginePrompt\",\n    outdir: str,\n    precision: str = \"autocast\",\n    record_step_images: bool = False,\n    output_file_extension: str = \"jpg\",\n    print_caption: bool = False,\n    make_gif: bool = False,\n    make_compare_gif: bool = False,\n    return_filename_type: str = \"generated\",\n    videogen: bool = False,\n):\n    \"\"\"\n    Generates and saves image files based on given prompts, with options for animations and videos.\n\n    Args:\n        prompts (list[ImaginePrompt] | ImaginePrompt): A prompt or list of prompts for image generation.\n        outdir (str): Directory path where the generated images and other files will be saved.\n        precision (str, optional): Precision mode for image generation, defaults to 'autocast'.\n        record_step_images (bool, optional): If True, saves step-by-step images of the generation process, defaults to False.\n        output_file_extension (str, optional): File extension for output images, must be 'jpg' or 'png', defaults to 'jpg'.\n        print_caption (bool, optional): If True, prints captions on the generated images, defaults to False.\n        make_gif (bool, optional): If True, creates a GIF from the generation steps, defaults to False.\n        make_compare_gif (bool, optional): If True, creates a comparison GIF with initial and generated images, defaults to False.\n        return_filename_type (str, optional): Type of filenames to return, defaults to 'generated'.\n        videogen (bool, optional): If True, generates a video from the generated images, defaults to False.\n\n    Returns:\n        list[str]: A list of filenames of the generated images.\n\n    Raises:\n        ValueError: If the output file extension is not 'jpg' or 'png'.\n    \"\"\"\n    from PIL import ImageDraw\n\n    from imaginairy.utils import get_next_filenumber, prompt_normalized\n\n    generated_imgs_path = os.path.join(outdir, \"generated\")\n    os.makedirs(generated_imgs_path, exist_ok=True)\n\n    base_count = get_next_filenumber(generated_imgs_path)\n    output_file_extension = output_file_extension.lower()\n    if output_file_extension not in {\"jpg\", \"png\"}:\n        raise ValueError(\"Must output a png or jpg\")\n\n    if not isinstance(prompts, list):\n        prompts = [prompts]\n\n    def _record_step(img, description, image_count, step_count, prompt):\n        steps_path = os.path.join(outdir, \"steps\", f\"{base_count:08}_S{prompt.seed}\")\n        os.makedirs(steps_path, exist_ok=True)\n        filename = f\"{base_count:08}_S{prompt.seed}_{image_count:04}_step{step_count:03}_{prompt_normalized(description)[:40]}.jpg\"\n\n        destination = os.path.join(steps_path, filename)\n        draw = ImageDraw.Draw(img)\n        draw.text((10, 10), str(description))\n        img.save(destination)\n\n    if make_gif:\n        for p in prompts:\n            p.collect_progress_latents = True\n    result_filenames = []\n    for result in imagine(\n        prompts,\n        precision=precision,\n        debug_img_callback=_record_step if record_step_images else None,\n        add_caption=print_caption,\n    ):\n        primary_filename = save_image_result(\n            result,\n            base_count,\n            outdir=outdir,\n            output_file_extension=output_file_extension,\n            primary_filename_type=return_filename_type,\n            make_gif=make_gif,\n            make_compare_gif=make_compare_gif,\n        )\n        if not primary_filename:\n            continue\n        result_filenames.append(primary_filename)\n        if primary_filename and videogen:\n            from imaginairy.api.video_sample import generate_video\n\n            try:\n                generate_video(\n                    input_path=primary_filename,\n                )\n            except FileNotFoundError as e:\n                logger.error(str(e))\n                exit(1)\n\n        base_count += 1\n        del result\n\n    return result_filenames\n</code></pre>"},{"location":"docs/Python/imagine/","title":"imagine()","text":"<p>Generates images based on the provided prompts using the ImaginAIry API.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[ImaginePrompt] | str | ImaginePrompt</code> <p>A prompt or list of prompts for image generation. Can be a string, a single ImaginePrompt instance, or a list of ImaginePrompt instances.</p> required <code>precision</code> <code>str</code> <p>The precision mode for image generation, defaults to 'autocast'.</p> <code>'autocast'</code> <code>debug_img_callback</code> <code>Callable</code> <p>Callback function for debugging images, defaults to None.</p> <code>None</code> <code>progress_img_callback</code> <code>Callable</code> <p>Callback function called at intervals with progress images, defaults to None.</p> <code>None</code> <code>progress_img_interval_steps</code> <code>int</code> <p>Number of steps between each progress image callback, defaults to 3.</p> <code>3</code> <code>progress_img_interval_min_s</code> <code>float</code> <p>Minimum seconds between each progress image callback, defaults to 0.1.</p> <code>0.1</code> <code>half_mode</code> <p>If set, determines whether to use half precision mode for image generation, defaults to None.</p> <code>None</code> <code>add_caption</code> <code>bool</code> <p>Flag to add captions to the generated images, defaults to False.</p> <code>False</code> <code>unsafe_retry_count</code> <code>int</code> <p>Number of retries for generating an image if it is deemed unsafe, defaults to 1.</p> <code>1</code> <p>Yields:</p> Type Description <p>The generated image(s) based on the provided prompts.</p> Source code in <code>imaginairy/api/generate.py</code> <pre><code>def imagine(\n    prompts: \"list[ImaginePrompt] | str | ImaginePrompt\",\n    precision: str = \"autocast\",\n    debug_img_callback: Callable | None = None,\n    progress_img_callback: Callable | None = None,\n    progress_img_interval_steps: int = 3,\n    progress_img_interval_min_s=0.1,\n    half_mode=None,\n    add_caption: bool = False,\n    unsafe_retry_count: int = 1,\n):\n    \"\"\"\n    Generates images based on the provided prompts using the ImaginAIry API.\n\n    Args:\n        prompts (list[ImaginePrompt] | str | ImaginePrompt): A prompt or list of prompts for image generation.\n            Can be a string, a single ImaginePrompt instance, or a list of ImaginePrompt instances.\n        precision (str, optional): The precision mode for image generation, defaults to 'autocast'.\n        debug_img_callback (Callable, optional): Callback function for debugging images, defaults to None.\n        progress_img_callback (Callable, optional): Callback function called at intervals with progress images, defaults to None.\n        progress_img_interval_steps (int, optional): Number of steps between each progress image callback, defaults to 3.\n        progress_img_interval_min_s (float, optional): Minimum seconds between each progress image callback, defaults to 0.1.\n        half_mode: If set, determines whether to use half precision mode for image generation, defaults to None.\n        add_caption (bool, optional): Flag to add captions to the generated images, defaults to False.\n        unsafe_retry_count (int, optional): Number of retries for generating an image if it is deemed unsafe, defaults to 1.\n\n    Yields:\n        The generated image(s) based on the provided prompts.\n    \"\"\"\n    import torch.nn\n\n    from imaginairy.schema import ImaginePrompt\n    from imaginairy.utils import (\n        check_torch_version,\n        fix_torch_group_norm,\n        fix_torch_nn_layer_norm,\n        get_device,\n    )\n\n    check_torch_version()\n\n    prompts = [ImaginePrompt(prompts)] if isinstance(prompts, str) else prompts\n    prompts = [prompts] if isinstance(prompts, ImaginePrompt) else prompts\n\n    try:\n        num_prompts = str(len(prompts))\n    except TypeError:\n        num_prompts = \"?\"\n\n    if get_device() == \"cpu\":\n        logger.warning(\"Running in CPU mode. It's gonna be slooooooow.\")\n        from imaginairy.utils.torch_installer import torch_version_check\n\n        torch_version_check()\n\n    if half_mode is None:\n        half_mode = \"cuda\" in get_device() or get_device() == \"mps\"\n\n    with torch.no_grad(), fix_torch_nn_layer_norm(), fix_torch_group_norm():\n        for i, prompt in enumerate(prompts):\n            concrete_prompt = prompt.make_concrete_copy()\n            prog_text = f\"{i + 1}/{num_prompts}\"\n            logger.info(f\"\ud83d\uddbc  {prog_text} {concrete_prompt.prompt_description()}\")\n            # Determine which generate function to use based on the model\n            if (\n                concrete_prompt.model_architecture\n                and concrete_prompt.model_architecture.name.lower() == \"flux\"\n            ):\n                from imaginairy.api.generate_flux import (\n                    generate_single_image as generate_single_flux_image,\n                )\n\n                generate_func = generate_single_flux_image\n            else:\n                from imaginairy.api.generate_refiners import (\n                    generate_single_image as generate_single_image_refiners,\n                )\n\n                generate_func = generate_single_image_refiners\n\n            for attempt in range(unsafe_retry_count + 1):\n                if attempt &gt; 0 and isinstance(concrete_prompt.seed, int):\n                    concrete_prompt.seed += 100_000_000 + attempt\n                result = generate_func(\n                    concrete_prompt,\n                    debug_img_callback=debug_img_callback,\n                    progress_img_callback=progress_img_callback,\n                    progress_img_interval_steps=progress_img_interval_steps,\n                    progress_img_interval_min_s=progress_img_interval_min_s,\n                    add_caption=add_caption,\n                    dtype=torch.float16 if half_mode else torch.float32,\n                    output_perf=True,\n                )\n                if not result.safety_score or not result.safety_score.is_filtered:\n                    break\n                if attempt &lt; unsafe_retry_count:\n                    logger.info(\"    Image was unsafe, retrying with new seed...\")\n\n            yield result\n</code></pre>"},{"location":"docs/Python/upscale/","title":"upscale()","text":"<p>Upscales an image using a specified super-resolution model.</p> <p>It accepts an image in various forms: a LazyLoadingImage instance, a PIL Image, or a string representing a URL or file path. Supports different upscaling models, customizable tile size, padding, and the number of repetitions for upscaling. It can use tiles to manage memory usage on large images and supports multiple passes for upscaling.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>LazyLoadingImage | Image | str</code> <p>The input image.</p> required <code>upscale_model</code> <code>str</code> <p>Upscaling model to use. Defaults to realesrgan-x2-plus</p> <code>DEFAULT_UPSCALE_MODEL</code> <code>tile_size</code> <code>int</code> <p>Size of the tiles used for processing the image. Defaults to 512.</p> <code>512</code> <code>tile_pad</code> <code>int</code> <p>Padding size for each tile. Defaults to 50.</p> <code>50</code> <code>repetition</code> <code>int</code> <p>Number of times the upscaling is repeated. Defaults to 1.</p> <code>1</code> <code>device</code> <p>The device (CPU/GPU) to be used for computation. If None, the best available device is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The upscaled image as a PIL Image object.</p> Source code in <code>imaginairy/api/upscale.py</code> <pre><code>def upscale_image(\n    img: \"Union[LazyLoadingImage, Image.Image, str]\",\n    upscale_model: str = DEFAULT_UPSCALE_MODEL,\n    tile_size: int = 512,\n    tile_pad: int = 50,\n    repetition: int = 1,\n    device=None,\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Upscales an image using a specified super-resolution model.\n\n    It accepts an image in various forms: a LazyLoadingImage instance, a PIL Image,\n    or a string representing a URL or file path. Supports different upscaling models, customizable tile size, padding,\n    and the number of repetitions for upscaling. It can use tiles to manage memory usage on large images and supports multiple passes for upscaling.\n\n    Args:\n        img (LazyLoadingImage | Image.Image | str): The input image.\n        upscale_model (str, optional): Upscaling model to use. Defaults to realesrgan-x2-plus\n        tile_size (int, optional): Size of the tiles used for processing the image. Defaults to 512.\n        tile_pad (int, optional): Padding size for each tile. Defaults to 50.\n        repetition (int, optional): Number of times the upscaling is repeated. Defaults to 1.\n        device: The device (CPU/GPU) to be used for computation. If None, the best available device is used.\n\n    Returns:\n        Image.Image: The upscaled image as a PIL Image object.\n    \"\"\"\n    from PIL import Image\n\n    from imaginairy.enhancers.upscale import upscale_image\n    from imaginairy.schema import LazyLoadingImage\n\n    if isinstance(img, str):\n        if img.startswith(\"https://\"):\n            img = LazyLoadingImage(url=img)\n        else:\n            img = LazyLoadingImage(filepath=img)\n    elif isinstance(img, Image.Image):\n        img = LazyLoadingImage(img=img)\n\n    return upscale_image(\n        img,\n        upscale_model,\n        tile_size=tile_size,\n        tile_pad=tile_pad,\n        repetition=repetition,\n        device=device,\n    )\n</code></pre>"}]}